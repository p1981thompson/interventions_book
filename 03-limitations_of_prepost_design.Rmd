
# Limitations of the pre-post design: biases related to systematic change
```{r packages, echo=F, message=F, warning=F}
list_of_packages<-c("tidyverse","kableExtra","knitr")
new.packages <- list_of_packages[!(list_of_packages %in% installed.packages()[,"Package"])]
if(length(new.packages))install.packages(new.packages,dependencies = TRUE)

library(tidyverse)
library(kableExtra)
library(knitr)
```
At first glance, assessing an intervention seems easy. Having used the information in chapter 2 to select appropriate measures, you administer these to a group of people before starting the intervention, and again after it is completed, and then look to see if there has been meaningful change. This is what we call a pre-post design, and it almost always gives a misleadingly rosy impression of how effective the intervention is. The limitations of such studies have been well-understood in medicine for decades, yet in other fields they persist, perhaps because they typically give results that look good.

Figure x shows some real data from a study conducted by Bishop, Adams and Rosen (2006), in which children were trained in a computerised game that involved listening to sentences and moving objects on a computer screen to match what they heard. The training items assessed understanding of word order, in sentences such as 'the book is above the cup', or 'the girl is being kicked by the boy'. There were two treatment conditions, but the difference between them is not important for the point we want to make, which is that there was substantial improvement on a language comprehension test administered at baseline and again after the intervention in both groups. If this was the only data available, then we might have been tempted to conclude that the intervention was effective. However, we also had data from a third group who just had 'teaching as usual' in their classroom. They showed just as much improvement as the groups doing the intervention, indicating that whatever was causing the improvement, it wasn't the intervention. 

To understand what was going on in this study, we need to recognise that there are several reasons why scores may improve after an intervention that are nothing to do with the intervention. These form the systematic biases that we mentioned in Chapter 1, and they can be divided into three kinds:

-	Spontaneous improvement
-	Practice effects
-	Regression to the mean

## Spontaneous improvement

We encountered spontaneous improvement in Chapter 1, where it provided a plausible reason for improved test scores in all three vignettes. People with acquired aphasia may continue to show recovery for months if not years after the brain injury, regardless of any intervention. Some toddlers who are 'late talkers' suddenly take off after a late start and catch up with their peers. And children in general get better at doing things as they get older. This is true not just for the more striking cases of late bloomers, but also for more serious and persistent problems. Children with autism spectrum disorder or severe comprehension problems usually do improve over time – it's just that the improvement may start from a very low baseline, so they fail to 'catch up' with their peer group.

Most of the populations that SLTs work with will show some spontaneous improvement which must be taken into account when evaluating intervention. Failure to recognise this is one of the factors that keeps snake-oil merchants in business: there are numerous unevidenced treatments on offer for conditions like autism and dyslexia. Desperate parents, upset to see their children struggling, subscribe to these. Over time, the child's difficulties start to lessen, and this is attributed to the intervention, creating more 'satisfied cases' that can then be used to advertise the intervention to others. 

As shown in Figure 3.1, one corrective to this way of thinking is to include a control group who either get no intervention or 'treatment as usual'. If these cases do as well as the intervention group, we start to see that the intervention is not as it seems. 

## Practice effects

The results that featured in Figure 3.1 are hard to explain just in terms of spontaneous change. The children in this study had severe and persistent language problems for which they were receiving special education, and the time lag between initial and final testing was relatively short. So maturational change seemed an unlikely explanation for the improvement. However, practice effects were much more plausible. 

A practice effect, as its name implies, is when you get better at doing a task simply because of prior exposure to the task. It doesn't mean you have to explicitly practice doing the task – rather it means that you can learn something useful about the task from previous exposure. One of our favourite illustrations of this is a balancing device that came as part of an exercise programme called Wii-fit. This connected with a controller box and the TV, so you could see exercises and try specific tests that were shown on the screen. Users were encouraged to do the exercises to estimate their 'brain age'. When the first author first tried the exercises, her brain age was estimated around 70 – at the time about 20 years more than her chronological age. But just a few days later, when she tried again, her brain age had improved enormously to be some 5 years younger than my chronological age. How could this be? She had barely begun to use the Wii-fit and so the change could not be attributed to the exercises. Rather, it was that familiarity with the evaluation tasks meant that she understood what she was supposed to do and could respond faster and apply strategies to optimise her performance. 

It is often assumed that practice effects don't apply to psychometric tests, especially those with high test-retest reliability. However, that doesn't follow. High reliability just tells you whether the rank ordering of a group of people will be similar from one test occasion to another – it does not say anything about whether the average performance will improve. In fact, we now know that some of our most reliable IQ tests show substantial practice effects persisting over many years. (Rabbitt studies ). One way of attempting to avoid practice effects is to use 'parallel forms' of a test – that is different items with the same format. Yet that does not necessarily prevent practice effects, if these depend primarily on familiarisation with task format and development of strategies. 

There is a simple way to avoid confusing practice effects with genuine intervention effects, and it's the same solution as for spontaneous improvement – include a control group who don't receive the intervention. They should show exactly the same practice effects as the treated group, and so provide a comparison against which intervention-related change can be assessed.

## Regression to the mean

Spontaneous improvement and practice effects are relatively easy to grasp intuitively using familiar examples. Regression to the mean is quite different – it actually appears counter-intuitive to most people and it is frequently misunderstood.

It refers to the phenomenon whereby, if you select a group on the basis of poor scores on a measure, X, then the worse the performance is at the start, the greater the improvement you can expect on re-testing. The key to understanding regression to the mean is to recognise two conditions that are responsible for it: a) the same measure is used to select cases for the study and to assess their progress and b) the measure has imperfect test-retest reliability. 

Perhaps the easiest way to get a grasp of what it entails is to suppose we had a group of 10 people and asked them each to throw a dice 10 times and total the score.   Let's then divide them into the 5 people who got the lowest scores and the 5 who got the highest scores and repeat the experiment. What do we expect? Well, assuming we don't believe that anything other than chance affects scores (no supernatural forces or 'winning streaks'), we'd expect the average score of the low-scorers to improve, and the average score of the high scorers to decline.  This is because the probability for any one person is that they will get an average score on any one set of dice throws.  So that's the simplest case, when we have a score that is determined **only** by chance: i.e. the test-retest reliability of the dice score is zero.

Cognitive test scores are interesting here because they are typically thought of comprising two parts: a 'true' score, which reflects how good you really are at whatever is being measured, and an 'error' score, which reflects random influences. Suppose, for instance, that you test a child's reading ability. In practice, the child is a very good reader, in the top 10% for her age, but the exact number of words she gets right will depend on all sorts of things: the particular words selected for the test (she may know 'catacomb' but not 'pharynx'), the mood she is in on the day, whether she is lucky or unlucky at guessing at words she is uncertain about. All these factors would be implicated in the 'error' score, which is treated just like an additional chance factor or throw of the dice affecting a test score.  A good test is mostly determined by the 'true' score, with only a very small contribution of an 'error' score, and we can identify it by the fact that children will be ranked by the test in a very similar way from one test occasion to the next, i.e. there will be good test-retest reliability.  In other words, the correlation from time 1 to time 2 will be high.

<!--**Possibly find a better figure?!**-->

```{r reg2mean,echo=FALSE}
set.seed(123)
mycor <- 0 #correlation between time 1 and time 2.
test_t1<-rnorm(n = 21, mean = 0, sd = 1) 
    
test_t2<-mycor*sd(test_t1)*test_t1+(sqrt(1-mycor^2))*sd(test_t1)*rnorm(length(test_t1))

groups<-rep("low",21)
for(i in 1:21){
if(test_t1[i]>=qnorm(.333)){groups[i]<-"medium"}
if(test_t1[i]>=qnorm(.667)){groups[i]<-"high"}  
}

kable(table(groups))

mydata<-data.frame(id=1:21,T1_test=test_t1,T2_test=test_t2,Group=groups)
mydata$id<-as.factor(mydata$id)
mydata$Group<-as.factor(mydata$Group)

#colMeans(mydata[,c('T1_test','T2_test')])

equal_means<-t.test(mydata$T1_test,mydata$T2_test)

ggplot(data=mydata,aes(y=T1_test,x=T2_test))+geom_point(aes(colour=Group),size=2)+geom_smooth(method='lm',formula=y~x,se=F)+geom_abline(slope=1,intercept=0)+theme_bw()+theme(legend.position = "top")

mytable<-as.data.frame(matrix(NA,nrow=3,ncol=3))

mytable[1,2:3]<-colMeans(mydata[mydata$Group=="low",c('T1_test','T2_test')])
mytable[2,2:3]<-colMeans(mydata[mydata$Group=="medium",c('T1_test','T2_test')])
mytable[3,2:3]<-colMeans(mydata[mydata$Group=="high",c('T1_test','T2_test')])

colnames(mytable) <- c("Group", "T1_Mean","T2_Mean")
mytable[1:3,1] <- c("Low","Medium","High")

knitr::kable(mytable) 


T2_group_means<-summary(aov(T2_test~Group, data=mydata))

```


*Simulated test scores for 21 children on tests varying in reliability. These figures show simulated scores for a group of 21 children on tests that vary in test-retest reliability.  In each case individuals are colour-coded depending on whether they fall in the bottom (blue), middle (purple) or top (red) third of scores at time 1. The simulations assume no systematic differences between time 1 and 2 - i.e. no intervention effect, maturation or practice.  Scores are simulated as random numbers at time 1, with time 2 scores then set to give a specific correlation between time 1 and 2, with no change in average score for the group as a whole.*

Now suppose we select children because they have a particularly low score at time 1. Insofar as chance contributes to their scores, then at time 2, we would expect the average score of such a group to improve, because chance pushes the group average towards the overall mean score.  The left-hand panel shows the situation when reliability (i.e., correlation between time 1 and time 2 scores) is zero, so scores are determined just by chance, like throwing a dice. The mean scores for the blue, purple and red cases at time 1 are, by definition different (they are selected to be low, medium and high scorers). But at time 2 they are all equivalent.  The upshot is that the mean at time 2 for those with initial low scores (blue) goes up, whereas for those that start out with high scores, the time 2 mean comes down.
 
The middle and right-hand panels show a more realistic situation, where the test score is mixture of true score and error. With very high reliability (right-hand panel) the effect of regression to the mean is small, but with medium reliability (middle panel) it is detectable by eye even for this very small sample.

The key point here is that if we select individuals on the basis of low scores on a test (as is often done, e.g. when identifying children with poor reading scores for a study of a dyslexia treatment), then, unless we have a highly reliable test with a negligible error term, the expectation is that the group's average score will improve on a second test session, for purely statistical reasons.  In general, psychometric tests are designed to have reasonable reliability, but this varies from test to test and is seldom higher than .75-.8.

So regression to the mean is a real issue in longitudinal studies.  It is yet another reason why scores will change over time. Zhang and Tomblin (2003) noted that we can overcome this problem by using different tests to select cases for an intervention study and to measure their improvement.  Or we can allow for regression to the mean if our study includes a control group, who will be subject to the same regression to the mean as the intervention group.

## Class exercise

Identify an intervention study on a topic of interest to you – you could do this by scanning through a journal, or by typing relevant keywords into a database such as Google Scholar, Web of Science or Scopus. If you are not sure how to do this, your librarian should be able to advise. It is important that the published article is available so you can read the detailed account. If an article is behind a paywall, you can usually obtain a copy by emailing the corresponding author.

Your task is to evaluate the article in terms of how well it addresses the systematic biases covered in this chapter. Are the results likely to be affected by spontaneous improvement, practice effects, or regression to the mean? Does the study design control for these? Note that for this exercise you are not required to evaluate the statistical analysis: the answers to these questions depend just on how the study was designed. 
