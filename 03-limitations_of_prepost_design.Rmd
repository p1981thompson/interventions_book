
# Limitations of the pre-post design: biases related to systematic change

```{r packages, echo=FALSE,message=FALSE,warning=FALSE}
list_of_packages<-c("tidyverse","kableExtra","knitr")
new.packages <- list_of_packages[!(list_of_packages %in% installed.packages()[,"Package"])]
if(length(new.packages))install.packages(new.packages,dependencies = TRUE)

library(tidyverse)
library(kableExtra)
library(knitr)
```

At first glance, assessing an intervention seems easy. Having used the information in chapter 2 to select appropriate measures, you administer these to a group of people before starting the intervention, and again after it is completed, and then look to see if there has been meaningful change. This is what we call a pre-post design, and it almost always gives a misleadingly rosy impression of how effective the intervention is. The limitations of such studies have been well-understood in medicine for decades, yet in other fields they persist, perhaps because they typically give results that look good.

```{r trogchangedata,echo=FALSE,message=FALSE,warning=FALSE,fig.cap='TROG data from a study conducted by @Bishop_2006',fig.width=8,fig.height=4}
require(reshape2)
# Data for means and SD from the article, time 1 and time 2 TROG
# First block, commented out, includes data from 'dropout' cases - originally plotted these as well
# time1<- c(64.92,11.77,66.92,11.63,69.33,10.22,55,0.001)
# time2 <- c(74.58,	13.13,	71.58,	13.32,	75.56,	13.08,	64.33,	8.08)
# group <- c('1. Speech','2. Modified speech','3. Untrained','Drop-out')
# myN <- c(12, 12,9, 3)
time1<- c(64.92,11.77,66.92,11.63,69.33,10.22)
time2 <- c(74.58,	13.13,	71.58,	13.32,	75.56,	13.08)
Group <- c('Normal Speech','Modified speech','Untrained')
myN <- c(12, 12,9)
Baseline<-time1[c(1,3,5)]
Post_intervention<-time2[c(1,3,5)]
pretestSE<-time1[c(2,4,6)]/sqrt(myN)
posttestSE <- time2[c(2,4,6)]/sqrt(myN)


df1 <- data.frame(Baseline, Post_intervention,Group)
dfse <-data.frame(pretestSE,posttestSE,Group)
df2 <- melt(df1, id.vars='Group')
df2se <-melt(dfse,id.vars='Group')
df2<-cbind(df2,df2se$value)
colnames(df2)[2:4]<-c('Occasion','TROG_Mean_scaled_score','se')
head(df2)

p <- ggplot(df2, aes(x=Group, y=TROG_Mean_scaled_score, fill=Occasion)) +
    geom_bar(stat='identity', position='dodge')+
  geom_errorbar(aes(ymin=TROG_Mean_scaled_score-se, ymax=TROG_Mean_scaled_score+se), width=.2,position=position_dodge(.9))+ylab('TROG Mean scaled score') 

p + coord_cartesian(ylim = c(50, 90)) +theme_bw()+ theme(axis.title.x = element_text(size=14), axis.text.x = element_text(size=14), legend.position = 'top',legend.text = element_text(size=10))

#Figure will need tidying up a bit! Would like bigger x axis label and underscores removed from y axis. 

```
Figure \@ref(fig:trogchangedata) shows some real data from a study conducted by @Bishop_2006, in which children were trained in a computerised game that involved listening to sentences and moving objects on a computer screen to match what they heard. The training items assessed understanding of word order, in sentences such as "the book is above the cup", or "the girl is being kicked by the boy". There were two treatment conditions, but the difference between them is not important for the point we want to make, which is that there was substantial improvement on a language comprehension test (Test for Reception of Grammar, TROG-2) administered at baseline and again after the intervention in both groups. If we only had data from these two groups, then we might have been tempted to conclude that the intervention was effective. However, we also had data from a third, untrained group who just had "teaching as usual" in their classroom. They showed just as much improvement as the groups doing the intervention, indicating that whatever was causing the improvement, it wasn't the intervention. 

To understand what was going on in this study, we need to recognise that there are several reasons why scores may improve after an intervention that are nothing to do with the intervention. These form the systematic biases that we mentioned in Chapter 1, and they can be divided into three kinds:

* Spontaneous improvement
*	Practice effects
*	Regression to the mean

## Spontaneous improvement

We encountered spontaneous improvement in Chapter 1, where it provided a plausible reason for improved test scores in all three vignettes. People with acquired aphasia may continue to show recovery for months or years after the brain injury, regardless of any intervention. Some toddlers who are "late talkers" suddenly take off after a late start and catch up with their peers. And children in general get better at doing things as they get older. This is true not just for the more striking cases of late bloomers, but also for more serious and persistent problems. Children with autism or severe comprehension problems usually do improve over time – it's just that the improvement may start from a very low baseline, so they fail to "catch up" with their peer group.

Most of the populations that SLTs work with will show some spontaneous improvement which must be taken into account when evaluating intervention. Failure to recognise this is one of the factors that keeps snake-oil merchants in business: there are numerous unevidenced treatments on offer for conditions like autism and dyslexia. Desperate parents, upset to see their children struggling, subscribe to these. Over time, the child's difficulties start to lessen, and this is attributed to the intervention, creating more "satisfied cases" that can then be used to advertise the intervention to others. 

As shown in Figure \@ref(fig:trogchangedata), one corrective to this way of thinking is to include a control group who either get no intervention or "treatment as usual". If these cases do as well as the intervention group, we start to see that the intervention is not as it seems. 

## Practice effects

The results that featured in Figure \@ref(fig:trogchangedata) are hard to explain just in terms of spontaneous change. The children in this study had severe and persistent language problems for which they were receiving special education, and the time lag between initial and final testing was relatively short. So maturational change seemed an unlikely explanation for the improvement. However, practice effects were much more plausible. 

A practice effect, as its name implies, is when you get better at doing a task simply because of prior exposure to the task. It doesn't mean you have to explicitly practice doing the task – rather it means that you can learn something useful about the task from previous exposure. A nice illustration of this is a balancing device that came as part of an exercise programme called Wii-fit. This connected with a controller box and the TV, so you could try specific exercises that were shown on the screen. Users were encouraged to start with exercises to estimate their "brain age". When the first author first tried the exercises, her brain age was estimated around 70 – at the time about 20 years more than her chronological age. But just a few days later, when she tried again, her brain age had improved enormously to be some 5 years younger than her chronological age. How could this be? She had barely begun to use the Wii-fit and so the change could not be attributed to the exercises. Rather, it was that familiarity with the evaluation tasks meant that she understood what she was supposed to do and could respond faster and apply strategies to optimise her performance. 

It is often assumed that practice effects don't apply to psychometric tests, especially those with high test-retest reliability. However, that doesn't follow. High reliability just tells you whether the rank ordering of a group of people will be similar from one test occasion to another – it does not say anything about whether the average performance will improve. In fact, we now know that some of our most reliable IQ tests show substantial practice effects persisting over many years [e.g., @Rabbitt_2004]. One way of attempting to avoid practice effects is to use "parallel forms" of a test – that is different items with the same format. Yet that does not necessarily prevent practice effects, if these depend primarily on familiarisation with task format and development of strategies. 

There is a simple way to avoid confusing practice effects with genuine intervention effects, and it's the same solution as for spontaneous improvement – include a control group who don't receive the intervention. They should show exactly the same practice effects as the treated group, and so provide a comparison against which intervention-related change can be assessed.

## Regression to the mean

Spontaneous improvement and practice effects are relatively easy to grasp intuitively using familiar examples. Regression to the mean is quite different – it actually appears counter-intuitive to most people and it is frequently misunderstood.

It refers to the phenomenon whereby, if you select a group on the basis of poor scores on a measure, X, then the worse the performance is at the start, the greater the improvement you can expect on re-testing. The key to understanding regression to the mean is to recognise two conditions that are responsible for it: a) the same measure is used to select cases for the study and to assess their progress and b) the measure has imperfect test-retest reliability. 

Perhaps the easiest way to get a grasp of what it entails is to suppose we had a group of 10 people and asked them each to throw a dice 10 times and total the score.   Let's then divide them into the 5 people who got the lowest scores and the 5 who got the highest scores and repeat the experiment. What do we expect? Well, assuming we don't believe that anything other than chance affects scores (no supernatural forces or "winning streaks"), we'd expect the average score of the low-scorers to improve, and the average score of the high scorers to decline.  This is because the most likely outcome for everyone is to get an average score on any one set of dice throws. So if you were below average on time 1, and average on time 2, your score has gone up; if you were above average on time 1, and average on time 2, your score has gone down.  So that's the simplest case, when we have a score that is determined **only** by chance: i.e. the test-retest reliability of the dice score is zero.

Cognitive test scores are interesting here because they are typically thought of comprising two parts: a "true" score, which reflects how good you really are at whatever is being measured, and an "error" score, which reflects random influences. Suppose, for instance, that you test a child's reading ability. In practice, the child is a very good reader, in the top 10% for her age, but the exact number of words she gets right will depend on all sorts of things: the particular words selected for the test (she may know "catacomb" but not "pharynx"), the mood she is in on the day, whether she is lucky or unlucky at guessing at words she is uncertain about. All these factors would be implicated in the "error" score, which is treated just like an additional chance factor or throw of the dice affecting a test score.  A good test is mostly determined by the "true" score, with only a very small contribution of an "error" score, and we can identify it by the fact that children will be ranked by the test in a very similar way from one test occasion to the next, i.e. there will be good test-retest reliability.  In other words, the correlation from time 1 to time 2 will be high.


```{r reg2mean,echo=FALSE, fig.cap='Simulated test scores for 240 children on tests varying in reliability. In each case individuals are colour-coded depending on whether they are given a test of low (.2), medium (.5) or high (.8) test-retest reliability. The simulations assume there is no effect of intervention , maturation or practice.'}
set.seed(12)
makecordat <- function(myN,mycor) {
  #mycor correlation between time 1 and time 2. ie test-retest reliability
test_t1<-rnorm(n = myN, mean = 0, sd = 1) 
    
test_t2<-mycor*sd(test_t1)*test_t1+(sqrt(1-mycor^2))*sd(test_t1)*rnorm(length(test_t1))
thisdat <- data.frame(cbind(test_t1,test_t2))
return(thisdat)
}

myN <- 80
allcor <- c(.2,.5,.8)
reliabgroup <-c('.2 Low','.5 Medium','.8 High')
for (c in 1:length(allcor)){
  mycor <- allcor[c]
  mydat <- makecordat(myN,mycor)
  mydat$reliab <- mycor
  mydat$reliabgroup <-reliabgroup[c]
  if (c==1)
  {bigdat<-mydat}
  if (c>1)
  {bigdat<-rbind(bigdat,mydat)}
}
colnames(bigdat)<-c('T1_test','T2_test','reliability','Test_reliability')
bigdat$Test_reliability <- as.factor(bigdat$Test_reliability)
p <- ggplot(data=bigdat,aes(x=T1_test,y=T2_test))+geom_point(aes(colour=Test_reliability),size=2)+theme_bw()+theme(legend.position = "top")
p+  geom_vline(xintercept =(-1),linetype="dashed", color = "darkgrey", size=1)+ylab('Test t2')+xlab('Test t1')+ theme(axis.title.y = element_text(size=14), axis.title.x = element_text(size=14), legend.position = 'top',legend.text = element_text(size=10))

#removed regression line +geom_smooth(method='lm',formula=y~x,se=F)
lowscorers <- bigdat[bigdat$T1_test< (-1),]


mymeans<-aggregate(lowscorers[,1:2],list(lowscorers$Test_reliability),mean)
colnames(mymeans)[1]<- 'Test_reliability'
mymeans$Test_reliability <- reliabgroup
mymeans[,2:3]<-round(mymeans[,2:3],2)

```


Figure \@ref(fig:reg2mean) shows simulated data for children seen on two occasions, when there is no effect of intervention, maturation or practice. The vertical grey line divides the samples into those who score more than one SD below the mean at time 1. Insofar as chance contributes to their scores, then at time 2, we would expect the average score of such a group to improve, because chance pushes the group average towards the overall mean score. One can see by inspection that those given a low-reliability test (red) tend to have scores close to zero at time 2, those given a high-reliability test (blue) tend to remain below average, whereas those given an medium-reliability test (green) are 
 intermediate. For children to the left of the grey vertical line, the mean time 1 scores for the three groups are very similar, but at time 2, the mean scores are `r mymeans[1,3]`, `r mymeans[2,3]` and `r mymeans[3,3]` respectively for the low, medium and high reliability tests. 

The key point here is that if we select individuals on the basis of low scores on a test (as is often done, e.g. when identifying children with poor reading scores for a study of a dyslexia treatment), then, unless we have a highly reliable test with a negligible error term, the expectation is that the group's average score will improve on a second test session, for purely statistical reasons. In general, psychometric tests are designed to have reasonable reliability, but this varies from test to test and is seldom higher than .75-.8.

Cynically, one could say that if you want to persuade the world that you have an effective treatment, just do a study where you select poor performers on a test with low reliability! There is a very good chance you will see a big improvement in their scores on re-test, regardless of what you do in between.

So regression to the mean is a real issue in longitudinal and intervention studies. It is yet another reason why scores will change over time. @Zhang_2003 noted that we can overcome this problem by using different tests to *select* cases for an intervention and to measure their outcomes.  Or we can allow for regression to the mean if our study includes a control group, who will be subject to the same regression to the mean as the intervention group.

## Class exercise

Identify an intervention study on a topic of interest to you – you could do this by scanning through a journal, or by typing relevant keywords into a database such as Google Scholar, Web of Science or Scopus. If you are not sure how to do this, your librarian should be able to advise. It is important that the published article is available so you can read the detailed account. If an article is behind a paywall, you can usually obtain a copy by emailing the corresponding author.

Your task is to evaluate the article in terms of how well it addresses the systematic biases covered in this chapter. Are the results likely to be affected by spontaneous improvement, practice effects, or regression to the mean? Does the study design control for these? Note that for this exercise you are not required to evaluate the statistical analysis: the answers to these questions depend just on how the study was designed.