<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 2 How to select an outcome measure | Evaluating and conducting intervention studies: a guide for speech and language therapists</title>
  <meta name="description" content="Introduction to methods for evaluating effectiveness of interventions in medical related fields">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 2 How to select an outcome measure | Evaluating and conducting intervention studies: a guide for speech and language therapists />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Introduction to methods for evaluating effectiveness of interventions in medical related fields" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 How to select an outcome measure | Evaluating and conducting intervention studies: a guide for speech and language therapists />
  
  <meta name="twitter:description" content="Introduction to methods for evaluating effectiveness of interventions in medical related fields" />
  

<meta name="author" content="Dorothy V.M. Bishop and Paul A. Thompson">


<meta name="date" content="2019-02-04">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="how-can-we-know-if-weve-made-a-difference.html">
<link rel="next" href="limitations-of-the-pre-post-design-biases-related-to-systematic-change.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Evaluating and conducting intervention studies: a guide for speech and language therapists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-did-i-write-this-book"><i class="fa fa-check"></i>Why did I write this book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-is-this-book-for"><i class="fa fa-check"></i>Who is this book for?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#what-is-covered"><i class="fa fa-check"></i>What is covered?</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="how-can-we-know-if-weve-made-a-difference.html"><a href="how-can-we-know-if-weve-made-a-difference.html"><i class="fa fa-check"></i><b>1</b> How can we know if weâ€™ve made a difference?</a><ul>
<li class="chapter" data-level="1.1" data-path="how-can-we-know-if-weve-made-a-difference.html"><a href="how-can-we-know-if-weve-made-a-difference.html#the-randomness-of-everything"><i class="fa fa-check"></i><b>1.1</b> The randomness of everything</a></li>
<li class="chapter" data-level="1.2" data-path="how-can-we-know-if-weve-made-a-difference.html"><a href="how-can-we-know-if-weve-made-a-difference.html#systematic-error"><i class="fa fa-check"></i><b>1.2</b> Systematic error</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="how-to-select-an-outcome-measure.html"><a href="how-to-select-an-outcome-measure.html"><i class="fa fa-check"></i><b>2</b> How to select an outcome measure</a><ul>
<li class="chapter" data-level="2.1" data-path="how-to-select-an-outcome-measure.html"><a href="how-to-select-an-outcome-measure.html#reliability"><i class="fa fa-check"></i><b>2.1</b> Reliability</a></li>
<li class="chapter" data-level="2.2" data-path="how-to-select-an-outcome-measure.html"><a href="how-to-select-an-outcome-measure.html#sensitivity"><i class="fa fa-check"></i><b>2.2</b> Sensitivity</a></li>
<li class="chapter" data-level="2.3" data-path="how-to-select-an-outcome-measure.html"><a href="how-to-select-an-outcome-measure.html#measuring-the-right-thing-validity"><i class="fa fa-check"></i><b>2.3</b> Measuring the right thing: validity</a></li>
<li class="chapter" data-level="2.4" data-path="how-to-select-an-outcome-measure.html"><a href="how-to-select-an-outcome-measure.html#functional-outcomes-vs-test-scores"><i class="fa fa-check"></i><b>2.4</b> Functional outcomes vs test scores</a></li>
<li class="chapter" data-level="2.5" data-path="how-to-select-an-outcome-measure.html"><a href="how-to-select-an-outcome-measure.html#subjectivity-as-a-source-of-bias"><i class="fa fa-check"></i><b>2.5</b> Subjectivity as a source of bias</a></li>
<li class="chapter" data-level="2.6" data-path="how-to-select-an-outcome-measure.html"><a href="how-to-select-an-outcome-measure.html#is-it-practical"><i class="fa fa-check"></i><b>2.6</b> Is it practical?</a></li>
<li class="chapter" data-level="2.7" data-path="how-to-select-an-outcome-measure.html"><a href="how-to-select-an-outcome-measure.html#class-exercise"><i class="fa fa-check"></i><b>2.7</b> Class exercise</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="limitations-of-the-pre-post-design-biases-related-to-systematic-change.html"><a href="limitations-of-the-pre-post-design-biases-related-to-systematic-change.html"><i class="fa fa-check"></i><b>3</b> Limitations of the pre-post design: biases related to systematic change</a><ul>
<li class="chapter" data-level="3.1" data-path="limitations-of-the-pre-post-design-biases-related-to-systematic-change.html"><a href="limitations-of-the-pre-post-design-biases-related-to-systematic-change.html#spontaneous-improvement"><i class="fa fa-check"></i><b>3.1</b> Spontaneous improvement</a></li>
<li class="chapter" data-level="3.2" data-path="limitations-of-the-pre-post-design-biases-related-to-systematic-change.html"><a href="limitations-of-the-pre-post-design-biases-related-to-systematic-change.html#practice-effects"><i class="fa fa-check"></i><b>3.2</b> Practice effects</a></li>
<li class="chapter" data-level="3.3" data-path="limitations-of-the-pre-post-design-biases-related-to-systematic-change.html"><a href="limitations-of-the-pre-post-design-biases-related-to-systematic-change.html#regression-to-the-mean"><i class="fa fa-check"></i><b>3.3</b> Regression to the mean</a></li>
<li class="chapter" data-level="3.4" data-path="limitations-of-the-pre-post-design-biases-related-to-systematic-change.html"><a href="limitations-of-the-pre-post-design-biases-related-to-systematic-change.html#class-exercise-1"><i class="fa fa-check"></i><b>3.4</b> Class exercise</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="improvement-due-to-nonspecific-effects-of-intervention.html"><a href="improvement-due-to-nonspecific-effects-of-intervention.html"><i class="fa fa-check"></i><b>4</b> Improvement due to nonspecific effects of intervention</a><ul>
<li class="chapter" data-level="4.1" data-path="improvement-due-to-nonspecific-effects-of-intervention.html"><a href="improvement-due-to-nonspecific-effects-of-intervention.html#placebo-effects-the-hawthorne-effect-and-the-rosenthal-effect"><i class="fa fa-check"></i><b>4.1</b> Placebo effects, the Hawthorne effect and the Rosenthal effect</a></li>
<li class="chapter" data-level="4.2" data-path="improvement-due-to-nonspecific-effects-of-intervention.html"><a href="improvement-due-to-nonspecific-effects-of-intervention.html#identifying-specific-intervention-effects-by-measures-of-mechanism"><i class="fa fa-check"></i><b>4.2</b> Identifying specific intervention effects by measures of mechanism</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="controlling-unwanted-effects-with-a-control-group.html"><a href="controlling-unwanted-effects-with-a-control-group.html"><i class="fa fa-check"></i><b>5</b> Controlling unwanted effects with a control group</a><ul>
<li class="chapter" data-level="5.1" data-path="controlling-unwanted-effects-with-a-control-group.html"><a href="controlling-unwanted-effects-with-a-control-group.html#is-it-ethical-to-include-a-control-group"><i class="fa fa-check"></i><b>5.1</b> Is it ethical to include a control group?</a><ul>
<li class="chapter" data-level="5.1.1" data-path="controlling-unwanted-effects-with-a-control-group.html"><a href="controlling-unwanted-effects-with-a-control-group.html#possible-adverse-effects-of-intervention"><i class="fa fa-check"></i><b>5.1.1</b> Possible adverse effects of intervention</a></li>
<li class="chapter" data-level="5.1.2" data-path="controlling-unwanted-effects-with-a-control-group.html"><a href="controlling-unwanted-effects-with-a-control-group.html#uncontrolled-studies-are-unethical"><i class="fa fa-check"></i><b>5.1.2</b> Uncontrolled studies are unethical</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="controlling-unwanted-effects-with-a-control-group.html"><a href="controlling-unwanted-effects-with-a-control-group.html#treated-vs-untreated-controls"><i class="fa fa-check"></i><b>5.2</b> Treated vs untreated controls</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="wait-list-controls-and-cross-over-designs.html"><a href="wait-list-controls-and-cross-over-designs.html"><i class="fa fa-check"></i><b>6</b> Wait-list controls and cross-over designs</a></li>
<li class="chapter" data-level="7" data-path="observational-studies.html"><a href="observational-studies.html"><i class="fa fa-check"></i><b>7</b> Observational studies</a><ul>
<li class="chapter" data-level="7.1" data-path="observational-studies.html"><a href="observational-studies.html#class-exercise-2"><i class="fa fa-check"></i><b>7.1</b> Class exercise</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="controlling-for-selection-bias-randomised-assignment-to-intervention.html"><a href="controlling-for-selection-bias-randomised-assignment-to-intervention.html"><i class="fa fa-check"></i><b>8</b> Controlling for selection bias: randomised assignment to intervention</a><ul>
<li class="chapter" data-level="8.1" data-path="controlling-for-selection-bias-randomised-assignment-to-intervention.html"><a href="controlling-for-selection-bias-randomised-assignment-to-intervention.html#units-of-analysis-individuals-vs-clusters"><i class="fa fa-check"></i><b>8.1</b> Units of analysis: Individuals vs clusters</a></li>
<li class="chapter" data-level="8.2" data-path="controlling-for-selection-bias-randomised-assignment-to-intervention.html"><a href="controlling-for-selection-bias-randomised-assignment-to-intervention.html#class-exercise-3"><i class="fa fa-check"></i><b>8.2</b> Class exercise</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-experimenter-as-a-source-of-bias.html"><a href="the-experimenter-as-a-source-of-bias.html"><i class="fa fa-check"></i><b>9</b> The experimenter as a source of bias</a><ul>
<li class="chapter" data-level="9.1" data-path="the-experimenter-as-a-source-of-bias.html"><a href="the-experimenter-as-a-source-of-bias.html#allocation-concealment"><i class="fa fa-check"></i><b>9.1</b> Allocation concealment</a></li>
<li class="chapter" data-level="9.2" data-path="the-experimenter-as-a-source-of-bias.html"><a href="the-experimenter-as-a-source-of-bias.html#the-importance-of-masking-for-assessments"><i class="fa fa-check"></i><b>9.2</b> The importance of masking for assessments</a></li>
<li class="chapter" data-level="9.3" data-path="the-experimenter-as-a-source-of-bias.html"><a href="the-experimenter-as-a-source-of-bias.html#conflict-of-interest"><i class="fa fa-check"></i><b>9.3</b> Conflict of interest</a></li>
<li class="chapter" data-level="9.4" data-path="the-experimenter-as-a-source-of-bias.html"><a href="the-experimenter-as-a-source-of-bias.html#class-exercise-4"><i class="fa fa-check"></i><b>9.4</b> Class exercise</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="further-potential-for-bias-who-drops-out-and-who-volunteers.html"><a href="further-potential-for-bias-who-drops-out-and-who-volunteers.html"><i class="fa fa-check"></i><b>10</b> Further potential for bias: who drops out and who volunteers?</a><ul>
<li class="chapter" data-level="10.1" data-path="further-potential-for-bias-who-drops-out-and-who-volunteers.html"><a href="further-potential-for-bias-who-drops-out-and-who-volunteers.html#dealing-with-dropouts-intention-to-treat-analysis"><i class="fa fa-check"></i><b>10.1</b> Dealing with dropouts: Intention to treat analysis</a></li>
<li class="chapter" data-level="10.2" data-path="further-potential-for-bias-who-drops-out-and-who-volunteers.html"><a href="further-potential-for-bias-who-drops-out-and-who-volunteers.html#who-volunteers-for-research"><i class="fa fa-check"></i><b>10.2</b> Who volunteers for research?</a></li>
<li class="chapter" data-level="10.3" data-path="further-potential-for-bias-who-drops-out-and-who-volunteers.html"><a href="further-potential-for-bias-who-drops-out-and-who-volunteers.html#class-exercise-5"><i class="fa fa-check"></i><b>10.3</b> Class exercise</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="putting-it-all-together-the-randomised-controlled-trial.html"><a href="putting-it-all-together-the-randomised-controlled-trial.html"><i class="fa fa-check"></i><b>11</b> Putting it all together: the randomised controlled trial</a><ul>
<li class="chapter" data-level="11.1" data-path="putting-it-all-together-the-randomised-controlled-trial.html"><a href="putting-it-all-together-the-randomised-controlled-trial.html#statistical-analysis-of-a-rct"><i class="fa fa-check"></i><b>11.1</b> Statistical analysis of a RCT</a></li>
<li class="chapter" data-level="11.2" data-path="putting-it-all-together-the-randomised-controlled-trial.html"><a href="putting-it-all-together-the-randomised-controlled-trial.html#obfuscation-and-omission-in-the-reporting-of-results"><i class="fa fa-check"></i><b>11.2</b> Obfuscation and omission in the reporting of results</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="yet-more-bias-distortions-arising-at-the-analysis-stage.html"><a href="yet-more-bias-distortions-arising-at-the-analysis-stage.html"><i class="fa fa-check"></i><b>12</b> Yet more bias: distortions arising at the analysis stage</a></li>
<li class="chapter" data-level="13" data-path="false-positives-and-false-negatives-are-we-missing-true-effects.html"><a href="false-positives-and-false-negatives-are-we-missing-true-effects.html"><i class="fa fa-check"></i><b>13</b> False positives and false negatives: are we missing true effects?</a></li>
<li class="chapter" data-level="14" data-path="drawbacks-of-the-rct.html"><a href="drawbacks-of-the-rct.html"><i class="fa fa-check"></i><b>14</b> Drawbacks of the RCT</a></li>
<li class="chapter" data-level="15" data-path="alternatives-to-rct-regression-discontinuity.html"><a href="alternatives-to-rct-regression-discontinuity.html"><i class="fa fa-check"></i><b>15</b> Alternatives to RCT: regression discontinuity</a><ul>
<li class="chapter" data-level="15.1" data-path="alternatives-to-rct-regression-discontinuity.html"><a href="alternatives-to-rct-regression-discontinuity.html#mediators"><i class="fa fa-check"></i><b>15.1</b> Mediators</a></li>
<li class="chapter" data-level="15.2" data-path="alternatives-to-rct-regression-discontinuity.html"><a href="alternatives-to-rct-regression-discontinuity.html#moderators"><i class="fa fa-check"></i><b>15.2</b> Moderators</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="alternatives-to-rct-within-subject-designs.html"><a href="alternatives-to-rct-within-subject-designs.html"><i class="fa fa-check"></i><b>16</b> Alternatives to RCT: within-subject designs</a><ul>
<li class="chapter" data-level="16.1" data-path="alternatives-to-rct-within-subject-designs.html"><a href="alternatives-to-rct-within-subject-designs.html#single-case-designs"><i class="fa fa-check"></i><b>16.1</b> Single case designs</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="practical-obstacles-to-the-ideal-study.html"><a href="practical-obstacles-to-the-ideal-study.html"><i class="fa fa-check"></i><b>17</b> Practical obstacles to the ideal study</a><ul>
<li class="chapter" data-level="17.1" data-path="practical-obstacles-to-the-ideal-study.html"><a href="practical-obstacles-to-the-ideal-study.html#sample-size-need-for-team-science"><i class="fa fa-check"></i><b>17.1</b> Sample size: need for team science?</a></li>
<li class="chapter" data-level="17.2" data-path="practical-obstacles-to-the-ideal-study.html"><a href="practical-obstacles-to-the-ideal-study.html#over-regulation-of-research-when-ethics-committees-misfire"><i class="fa fa-check"></i><b>17.2</b> Over-regulation of research: when ethics committees misfire</a></li>
<li class="chapter" data-level="17.3" data-path="practical-obstacles-to-the-ideal-study.html"><a href="practical-obstacles-to-the-ideal-study.html#problems-in-generalising-to-the-real-world"><i class="fa fa-check"></i><b>17.3</b> Problems in generalising to the real world</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="can-we-believe-the-literature-publication-bias.html"><a href="can-we-believe-the-literature-publication-bias.html"><i class="fa fa-check"></i><b>18</b> Can we believe the literature? Publication bias</a></li>
<li class="chapter" data-level="19" data-path="pre-registration-as-a-means-to-combat-publication-bias.html"><a href="pre-registration-as-a-means-to-combat-publication-bias.html"><i class="fa fa-check"></i><b>19</b> Pre-registration as a means to combat publication bias</a></li>
<li class="chapter" data-level="20" data-path="avoiding-waste-the-need-to-start-with-a-systematic-review.html"><a href="avoiding-waste-the-need-to-start-with-a-systematic-review.html"><i class="fa fa-check"></i><b>20</b> Avoiding waste: the need to start with a systematic review</a></li>
<li class="chapter" data-level="21" data-path="a-template-for-a-research-protocol.html"><a href="a-template-for-a-research-protocol.html"><i class="fa fa-check"></i><b>21</b> A template for a research protocol</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Evaluating and conducting intervention studies: a guide for speech and language therapists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="how-to-select-an-outcome-measure" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> How to select an outcome measure</h1>
<p>Suppose you want to evaluate the effectiveness of a parent-based intervention for improving communication in three-year-olds with poor language skills. You plan to assess their skills before the intervention, immediately after the intervention, and again six months later. The initial measurement period is known as the baseline â€“ because it acts against as a reference point against which improvement can be measured.</p>
<p>There are many measures you could choose: the childâ€™s mean length of utterance (MLU), scores on a direct assessment such as preschool CELF, the parentâ€™s response on a language inventory such as xxx. You may wonder whether you should include as many measures as possible to ensure you cover all bases. However, as we shall see in chapter x, if you measure too many things, you run the risk of getting spurious results, so it is important to specify a primary outcome measure â€“ the one you would put your money on as most likely to show the effect of interest, if you were a betting person.</p>
<p>The key questions you have to ask yourself are:</p>
<ol style="list-style-type: decimal">
<li>Is the measure reliable?</li>
<li>Is it sensitive?</li>
<li>Is it valid? i.e., does it measure what I want to measure?</li>
<li>Is it practical?</li>
</ol>
<div id="reliability" class="section level2">
<h2><span class="header-section-number">2.1</span> Reliability</h2>
<p>You may be surprised to see reliability at the top of the list. Surely validity is more important? Well, yes and no. As shown in Figure x, thereâ€™s not much point in having a measure that is reliable unless it is also valid. But a measure that is valid but not reliable is worse than useless in an intervention study, so I put reliability at the top of the list.
(targets figure here)</p>
<p>So what is reliability? This has to do with the issue of random error or â€˜noiseâ€™, which can be estimated by seeing how far scores on one testing occasion line up with scores on another occasion close in time (i.e.Â before we expect any change due to maturation or intervention). Quite simply you want a measure that reflects the childâ€™s underlying skill, where there is minimal influence from random, unwanted sources of variation. So if we have two occasions of measurement, they should be closely similar. The similarity can be quantified by a correlation coefficient, demonstrated in Figure x.</p>
<p><strong>Figure: The correlation coefficient is a statistic that takes the value of zero when there is no relationship between two variables, one whether there is a perfect relationship, and minus one when there is an inverse relationship. If you draw a straight line through the points on the graph, then if all points fall exactly on the line, the correlation is 1, indicating that you can predict perfectly a personâ€™s score on Y if you know their score on X.</strong></p>
<p>Letâ€™s illustrate this with mean length of utterance (MLU). Roger Brownâ€™s classic work first showed that in very young children this is a pretty good indicator of a childâ€™s language level (ref and ?plot). As children grow older, though, it become less useful, and it may be strongly influenced by the context in which language is sampled. Length of the language sample will also be important: if you just based a MLU estimate on ten child utterances, then the estimate will be less stable than if you took 50 utterances.</p>
<p>We canâ€™t say that MLU is inherently good or bad on the reliability front: it will depend on numerous factors. It may work well if assessed from a reasonable length of language sample collected in closely similar contexts. Reliability may be better for a clinical sample, where language skills are less variable, than for typically-developing children. The important thing is to find out as much as you can about reliability before committing to an outcome measure â€“ or even better, do your own test-retest study with the measures of interest before embarking on a study.</p>
<p>So how reliable should a measure be? Most psychometric tests report reliability estimates, and a good test is expected to have test-retest reliability of at least .8. But be careful in interpreting such estimates, as you need also to consider the age range on which the estimate is based. Figure x shows how a test that has very poor reliability within 3-year-olds looks highly reliable when considered across the full age range from 3 to 8 years. This is because the index of reliability, the correlation coefficient, is affected by the range of scores. If your study is focused just on 3-year-olds, you really want to know how reliable it is just within that age range.</p>
<p>The topic of reliability is covered more formally in test theory (ref). This involves a mathematical approach that treats an observed test score as the sum of a â€˜trueâ€™ effect (i.e.Â what you want to measure) plus random error. The lower the reliability, the greater the random error, and the harder it is to detect the true effect of intervention against the background of noise. Figure x illustrates this.
(illustrative figure)</p>
<p>I have focused on test-retest reliability as this is the most relevant in intervention studies. If you plan to use the same measure at baseline and after intervention, then what you need to know is how much variation in that measure is likely to occur just by chance. There are other reliability indices that are sometimes reported with psychometric tests. In particular split-half reliability and internal consistency (Cronbachâ€™s alpha), both of which consider the extent to which a score varies depending on the specific items used to calculate it. For instance, we could assess split half reliability for MLU by computing it separately for all the odd-numbered utterances and the even-numbered utterances. Although this gives useful information, it is usually overestimates test-retest reliability, because it does not take into account fluctuations in measurement that relate to changes in the context or the childâ€™s state.</p>
<p>It is much easier to compute measures of internal consistency than to do the extra testing that is needed to estimate test-retest reliability, and many published psychometric tests only provide that information. Table x shows reliability estimates from some commonly used tests.
(table</p>
</div>
<div id="sensitivity" class="section level2">
<h2><span class="header-section-number">2.2</span> Sensitivity</h2>
<p>Those who develop psychometric tests often focus on reliability and validity but neglect sensitivity. Yet sensitivity is a vital requirement for an outcome measure in an intervention study. This refers to the grain of the measurement: whether it can detect small changes in outcome. Consider Bridget Jones on a holiday to a remote place where there are no scales, just a primitive balance measure that allows her to compare herself against weights of different sizes (cartoon here). She would be unable to detect the daily fluctuations in pounds, and only be able to map her weight change in half-stone units. She could genuinely lose weight but be unaware of the fact.</p>
<p>Many standardized tests fall down on sensitivity, especially in relation to children scoring at the lower end of the ability range. It is customary to convert raw scores into scaled scores on these tests. This allows us to have a single number that can be interpreted in terms of how well the child is performing relative to others of the same age. But these often reduce a wide range of raw scores to a much smaller set of scaled scores, as illustrated in Table x. This means that a child could make substantial gains in raw score after intervention, but still come out with the same scaled score.
(Cathy Adams; Elspeth McCartney; EEF)</p>
<p>For this reason, it is often recommended that raw scores be used for evaluating intervention effects. This is fine if the study involves a narrow age range, but it is more problematic when a wider range is used, because age will be a major determinant of test score. Indeed, that is one reason why scaled scores are often preferred: they allow us to compare children of different ages on the same metric â€“ i.e.Â a score that reflects how the childâ€™s performance relates â€“ statistically â€“ to that of a normative group of the same age.</p>
<p>In the case where a wide age range is used, raw scores can work well provided an appropriate statistical analysis is performed to take into account the age differences. We will cover this topic in chapter x.</p>
<p>Problems with sensitivity can also be an issue with measures based on rating scales. For instance, if we just categorise children on a 5-point scale as â€˜well below averageâ€™, â€˜below averageâ€™, â€˜averageâ€™, â€˜above averageâ€™ or â€˜well above averageâ€™, we are stacking the odds against showing an intervention effect â€“ especially if our focus is on children who are in the bottom two categories to start with. Yet we also know that human raters are fallible and may not be able to make finer-grained distinctions. Some instruments may nevertheless be useful if they combine information from a set of ratings.</p>
<p>Although we need sensitive measures, we should not assume that a very fine-grained measure is always better than a coarser one. For instance, we may be measuring naming latency in aphasic patients as an index of improvement in word-finding. Itâ€™s unlikely that we need millisecond precision in the timing, because the changes of interest are likely to be in the order of tenths of a second at most. While thereâ€™s probably no harm in recording responses to the nearest millisecond, this is not likely to provide useful information.</p>
</div>
<div id="measuring-the-right-thing-validity" class="section level2">
<h2><span class="header-section-number">2.3</span> Measuring the right thing: validity</h2>
<p>A modification of a popular adage is â€˜If a thing is not worth doing, itâ€™s not worth doing well.â€™ This applies to selection of outcome measures: you could have a highly reliable and sensitive measure, but if it is not measuring the right thing, then thereâ€™s no point in using it.</p>
<p>Deciding what is the â€˜right thingâ€™ is an important part of designing any invention study, and it can be harder than it appears at first sight. The answer might be very different for different kinds of intervention. Iâ€™ll start with an issue that is particularly relevant to the first and third vignettes from chapter 1, word-finding intervention for aphasia, and the classroom-based vocabulary intervention</p>
<p>Generalisability of results: the concepts of far and near transfer
The vignettes on word-finding intervention and vocabulary training illustrate interventions that have a specific focus. This means we can potentially tie our outcome measures very closely to the intervention: we would want to measure speed of word-finding in the first case, and vocabulary size in the second.</p>
<p>There is a risk, though, that this approach would lead to trivial findings. If we did a word-finding training with an aphasic patient using ten common nouns and then showed that they his naming had speeded up on those same ten words, this might give us some confidence that the training approach worked (though we would need appropriate controls, as discussed in later chapters). However, ideally, we would want to the intervention to produce effects that generalised and improved his naming across the board. Similarly, showing that a teaching assistant can train children to learn ten new animal names is not negligible, but it doesnâ€™t tell us whether this approach has any broader benefits.</p>
<p>These issues can be important in situations such as phonological interventions, where there may be a focus on training the child to produce specific contrasts between speech sounds. If we show that they master those contrasts but not others, this may give us confidence that it was the training that had the effect, rather than spontaneous maturation (see Chapter x), but at the same time we might hope that training one contrast would have an impact on the childâ€™s phonological system and lead to improved production of other contrasts that were not directly trained.</p>
<p>These examples illustrate the importance of testing the impact of intervention not only on particular training targets, but also on other related items that were not trained. As noted above, this is something of a two-edged sword. We may hope that treatment effects will generalise, but if they do, it can be difficult to be certain that it was our intervention that brought about the change. The important thing when planning an intervention is to think about these issues and consider whether the mechanism targeted by the treatment is expected to produce generalised effects, and if so to test for those. This is discussed further in chapter 4.</p>
<p>The notion of generalisation assumes particular importance when the intervention does not directly target skills that are of direct relevance to everyday life. An example is CogMed training, which is a computer-based intervention that has been promoted as a way of improving childrenâ€™s working memory and intelligence. The child plays games that involve visual tasks that tax working memory, with difficulty increasing as performance improves. Early reports maintained that training on these tasks led to improvement on nonverbal intelligence, as assessed by Ravenâ€™s Matrices. However, more recent literature has challenged this claim, arguing that what is seen is â€˜near transferâ€™ â€“ i.e.Â improvement in the types of memory task that are trained â€“ without any evidence of â€˜far transferâ€™ â€“ i.e.Â improvement in other cognitive tasks. This is still a matter of hot debate, but it seems that many forms of â€˜computerised brain trainingâ€™ that are available commercially give disappointing results. If repeatedly doing computerised memory exercises only improves the ability to do those exercises, with no â€˜knock onâ€™ effects on everyday functioning, then the value of the intervention is questionable. It would seem preferable to use the time on training skills that would be useful in the classroom.</p>
</div>
<div id="functional-outcomes-vs-test-scores" class="section level2">
<h2><span class="header-section-number">2.4</span> Functional outcomes vs test scores</h2>
<p>In the second vignette we have an intervention where issues of far and near transfer are not relevant, as the intervention does not target specific aspects of language, but rather aims to modify the parental communicative style in order to provide a general boost to the childâ€™s language learning and functional communication. This suggests we need a rather general measure, and we are likely to consider using a standardized language test because this has the advantage of providing a reasonably objective and reliable approach to measurement. But does it measure the things that clients care about? Would we regard our intervention as a failure if the child made little progress on the standardized test, but was much more communicative and responsive to others? Or even if the intervention led to a more harmonious relationship between parent and child, but did not affect the childâ€™s language skills?</p>
<p>We might decide that these are the most important key outcomes, but then we have to establish how to measure them. In thinking about measures, it is important to be realistic about what one is hoping to achieve. If, for instance, the therapist is working with a client who has a chronic long-term problem, then the goal may be to help them use the communication skills they have to maximum effect, rather than to learn new language. The outcome measure in this case should be tailored to assess this functional outcome, rather than a gain on a measure of a specific language skill.</p>
</div>
<div id="subjectivity-as-a-source-of-bias" class="section level2">
<h2><span class="header-section-number">2.5</span> Subjectivity as a source of bias</h2>
<p>In chapters x and x I will discuss various sources of bias that can affect studies, but one that crops up at the measurement stage is the impact of so-called â€˜demand characteristicsâ€™ on subjective ratings. Consider, for a moment, how you respond when a waiter comes round to ask whether everything was okay with your meal. There are probably cultural differences in this, but the classic British response is to smile and say it is fine even if it was disappointing. We tend to adopt a kind of â€˜grade inflationâ€™ to many aspects of life when asked to rate them, especially if we know the person whose work we are rating.</p>
<p>In the context of intervention, people usually want to believe that interventions are effective and they donâ€™t want to appear critical of those administering the intervention, and so ratings of language are likely to improve from baseline to post-test, even if no real change has occurred. This phenomenon has been investigated particularly in situations where people are evaluating treatments that have cost them time and money (cognitive dissonance) but it is likely to apply even in experimental settings when interventions are being evaluated at no financial cost to those participating.</p>
<p>An example of this in the published literature comes from x et al who did a small-scale study to evaluate a computerised language intervention, FastForword (FFW). I will discuss larger evaluations of FFW in chapter x, but this study is noteworthy because as well as measuring childrenâ€™s language pre and post intervention, it included parent ratings of childrenâ€™s outcomes. There was a striking dissociation between the positive parent reports and the lack of improvement on language tests.
Another example comes from a well-conducted trial of â€˜Sunshine therapyâ€™ for children with a range of neurodevelopmental disorders; here again we see that parents were very positive about the intervention, while objective measures showed children had made not significant progress relative to a control group.</p>
<p>Such results are inherently ambiguous. It could be that parents are picking up on positive aspects of intervention that are not captured by the language tests. For instance, in the Sunshine therapy study, parents reported that their children had gained in confidence â€“ something that was not assessed by other means. However, there it is hard to know whether these evaluations are valid, as they are likely to be contaminated by demand characteristics.</p>
<p>Ideally we want measures that are valid indicators of things that are important for functional communication, yet are reasonably objective â€“ and they need also to be reliable and sensitive! I donâ€™t have simple answers as to how this can be achieved, but it is important for researchers to discuss these issues when designing studies to ensure they achieve optimal measures.</p>
</div>
<div id="is-it-practical" class="section level2">
<h2><span class="header-section-number">2.6</span> Is it practical?</h2>
<p>Intervention research is usually costly because of the time that is needed to recruit participants, run the intervention and do the assessments. There will always be pressures, therefore, to use assessments that are efficient, and provide key information in a relatively short space of time.</p>
<p>In my career I have occasionally been asked to advise people who are designing an intervention study, and I find that practicality is often very low on the list of topics that is discussed. A common experience is that the researchers want to measure everything they can think of in as much detail as possible. This is understandable: one does not want to pick the wrong measure and so miss an important impact of the intervention. But, as noted above, and discussed more in chapter x, there is a danger that too many measures will just lead to spurious findings. And each new measure will incur a time cost, which will ultimately translate to a financial cost, as well as potentially involving participants in additional assessment. There is, then, an ethical dimension to selection of measures: we need to optimise our selection of outcome measures to fulfil all the criteria of reliability, sensitivity and validity, but also to be as detailed and complex as we need, but no more.</p>
<p>My interest in efficiency of measurement may be illustrated with a vignette. When I started out in research, I was not involved in an intervention study, but I was embarking on a longitudinal study of 4-year-olds with developmental language disorders (Bishop &amp; Edmundson, 1987). I took advice on what measures to use, and a common piece of advice was that I should take a language sample, and then analyse it using LARSP (Crystal et al.,xxxx), which at the time was a popular approach to grammatical analysis. Fortunately, I also had the chance to meet Catherine Renfrew, a retired SLT who had developed some language assessments for this age range â€“ a sentence elicitation task, the Action Picture Test â€“ and a narrative task â€“ the Bus Story. As a practising therapist, Renfrew saw the need for short assessments that were easy to administer and interpret, and the tests she had developed fitted the bill.</p>
<p>I tried language sampling in my study but it seemed to provide little useful information in relation to the time it took to gather and transcribe the sample. Many of the children in my study rather little and did not attempt complex constructions. I found I could get more information in five minutes with the two Renfrew tests thank I could in 30 minutes of language sampling. Furthermore, I had more confidence in the test results, as all children were given the same task. When I came to do a grammatical analysis, I found that, after many hours of training in LARSP, analysing the results, and attempting to extract a quantitative measure from this process, I ended up with something that had a correlation of greater than .9 with mean length of utterance (MLU). The lesson I learned was that the measure needs to fit the purpose of what you are doing. I wanted an index of grammatical development that could be used to predict childrenâ€™s future progress. The Renfrew tasks provided to be among the most effective measures for doing that. A therapist working with a child might well find LARSP and language sampling useful for identifying therapy targets and getting a full picture of the childâ€™s abilities, but for my purposes this was far more detail than I needed.</p>
<p>There are other cases where researchers do very complex analysis in the hope that it might give a more sensitive indicator of language, only to find that it is highly intercorrelated with a much simpler index: use of xx index in nonword repetition is one example â€“ this score, which takes into account the specific errors made in nonword repetition, is highly correlated with a score based on number of nonwords correct. In the domain of expressive phonology, it was only after I tried to develop an index based on analysis of phonological processes that I found that this was entirely predictable from a much simpler measure of percentage consonants correct.</p>
<p>A related point is that researchers are often tempted by the allure of the new, especially when this is associated with fancy technology, such as methods of brain scanning or eye-tracking. Be warned: these approaches yield masses of data that are extremely complex to analyse, and they typically are not well-validated in terms of reliability, sensitivity or validity! Even when high-tech apparatus is not involved, the newer the measure, the less likely it is to be psychometrically established â€“ some measures of executive functioning fall in this category, as well as most measures that are derived from experimental paradigms. Clearly, there is an important place for research that uses these new techniques to investigate the nature of language disorders, but that place is not as outcome measures in intervention studies.</p>
<p>So, on the basis of my experience, I would advise that if you are tempted to use a complex, time-consuming measure, it is worthwhile first doing a study to see how far it is predictable from a more basic measure targeting the same process. It may save a lot of researcher time and we owe it to our research participants to do this due diligence to avoid subjecting them to unnecessarily protracted assessments.</p>
</div>
<div id="class-exercise" class="section level2">
<h2><span class="header-section-number">2.7</span> Class exercise</h2>
<p>Pick one of the three vignettes from Chapter 1 â€“ or if you prefer, pick another intervention scenario â€“ and make a list of possible outcome measures. For each one, evaluate its reliability, sensitivity, validity and practicality. At the end of this exercise, try to come up with a primary outcome measure that will be best for demonstrating an effect of intervention.</p>
<p>â€ƒ</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="how-can-we-know-if-weve-made-a-difference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="limitations-of-the-pre-post-design-biases-related-to-systematic-change.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["intervention book.pdf", "intervention book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
