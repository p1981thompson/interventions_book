---
output:
  pdf_document: default
  html_document: default
---


# How big a sample do I need? Statistical power, sampling and type II errors
```{r packages12, echo=F,warning=F,message=F}
list_of_packages<-c("tidyverse","kableExtra","knitr","MASS")
new.packages <- list_of_packages[!(list_of_packages %in% installed.packages()[,"Package"])]
if(length(new.packages))install.packages(new.packages,dependencies = TRUE)

library(tidyverse)
library(kableExtra)
library(knitr)
library(MASS)
library(dplyr)

```
### Sampling   
Consider the following scenario: You have five people who tried a weight loss program, Carbocut, which restricted carbohydrates, and who lost an average of 4 lb over 2 weeks. Another five people just tried sticking to three meals a day (Program 3M). They lost an average of 1 lb over 2 weeks. Would that convince you that carbohydrate restriction is superior?  What if we tell you that one person in the Carbocut group lost 6 lb and the other gained 2 lb; in the Program 3M group, one person lost 5 lb and one gained 3 lb. Few people would find this convincing evidence. With only two people per group, the average is quite unstable and highly dependent on the specific people in the group. Things would look  more promising if you had 1000 people in each group, and still saw a 4 lb loss vs a 2 lb loss.  Individual weights would fluctuate, but the average is a much better indication of what is typical when we have a larger sample, because individual people with extreme scores will have less impact.  

How could we know the real truth of just how much weight loss is associated with each diet? In theory, there is a way we could find out - we could recruit every single person in the world who meets some selection criterion for inclusion in the study, and randomly assign them to one or the other diet, and then measure weight loss. Of course, in practice, that is neither possible, ethical nor sensible. Rather, the aim of research is to test a group of people whose results can be generalised to the broader population. So we recruit a sample which is intended to represent that much larger population. As the  example above demonstrates, we need to think about sample size, as this will affect how much confidence we can place in the estimates from the sample. We know that two people is too small a sample. 1000 seems much better but, outside medical trials, is often too expensive and difficult to achieve. So how do we decide the optimal sample?  To answer that question, we first need to introduce the concept of **effect size**.  

### Effect size

Another way of thinking about our first example is that it illustrates that we can't interpret an average difference between two groups unless we know something about the variation within each group. In the example where we have only two people per group, the variation _within_ each group (8 lb difference between highest and lowest) is far greater than the variation _between_ groups (2 lb). 

```{r demo-variation,echo=F,include=T}
library(beeswarm) #nice plotting package
library(doBy)
set.seed(8) #to ensure same values generated on each run - change to any other number for different results

#create a set of values to simulate for true group differences in SD units
truediff<-c(.1,.75,1.25) #here we have 3 values; c stands for cocatenate so we get 3 values
#in a vector; we can refer to them as truediff[1], truediff[2] and truediff[3]
truelabel<-c('A','B','C') #to label the plots for each run

ndiffs<-length(truediff)

par(mfrow=c(1,3))

myn<-20 #N per group
myES <- NA
for (i in 1:ndiffs){
  thisdiff<-truediff[i]
mynum <- rnorm(myn*2)
mynum[1:myn]<-mynum[1:myn]-thisdiff  #add truediff value to group 1 scores

#scale so that variances differ rather than means
mynum<-(-2)*mynum/(i)-2 #just divide by 1, 2 or 3, after switching polarity and multiplying by 2 (to make it more compatible with the example)

mygp<-c(rep(2,myn),rep(1,myn))#generate group IDs for equal sized groups 1 and 2
mydat<-data.frame(cbind(mygp,mynum)) #stick it all together in a dataframe
#mydat #uncomment this to look at the dataframe if you like

#Show the plot with the stats
myylab<-''
if (i==1){myylab<-'Weight loss'}
mydat$mygp<-as.factor(mydat$mygp)
levels(mydat$mygp)<-c('Carbocut','3M')

beeswarm(mynum~mygp , data = mydat,xlab=truelabel[i],ylab=myylab,
         col='red',pch=16,ylim=c(-5,2),cex.axis=1.5,cex.lab=2)
segments(x0 = 0.7, x1 = 1.3,
         y0 = mean(mydat$mynum[21:40]), 
         lty = 1, lwd = 2,col='black')
segments(x0 = 1.7, x1 = 2.3,
         y0 = mean(mydat$mynum[1:20]), 
         lty = 1, lwd = 2,col='black')
myES[i]<-(mean(mydat$mynum[21:40])-mean(mydat$mynum[1:20]))/mean(sd(mydat$mynum[1:20]),sd(mydat$mynum[21:40]))

}



```




Figure x shows three different scenarios with 20 participants per group, where the mean difference in lbs of weight loss between the two diets is similar in each case, but the variation within each group is greatest in scenario A, and least in scenario C. _Relative to the variation within each group_, the difference in means is smallest in A, intermediate in B and largest in C. Another way of saying this is that the _effect size_ is different in the three scenarios.  

There are various ways of measuring effect size: a simple measure is Cohen's _d_, which is the difference between groups measured in standard deviation units. It is computed by dividing the mean difference between groups by the standard deviation. For conditions A, B and C, Cohen's _d_ is .4, .8 and 1, respectively.  Note that in all three scenarios, the distributions of weight loss in the two groups overlap. In every case, there are people in the 3M group with weight loss greater than the mean of the Carbocut group, as well as people in the Carbocut group with less weight loss than the mean of the 3M group. As shown in Figure z, the overlap is related to effect size: the larger the effect size, the less the overlap.  It can be sobering to note that for most effective educational and therapeutic interventions, effect sizes are typically no more than .3 or .4. Thus we have to pick out a meaningful signal - an intervention effect - from a very noisy background. 

![Figure z. Overlap in distributions of scores of two groups for different effect sizes (Cohen's _d_.](Effectsizes_fig.jpg)

### Sample size affects accuracy of estimates  

Once we have an idea of the likely effect size in our study, we can estimate how big a sample we need. If we have a small effect size, then we need many observations to get a reliable estimate of the true effect of intervention. Figure y resembles Figure x, but there is an important difference: in Figure y, each blob represents the _observed mean_ from a sample of a given size, taken from a population where the true effect size, shown as a dotted horizontal line, is 0.3. 

Notice how the observed means jump around from sample to sample when sample size is small, but become more stable once we have 80 or more participants per group.  When there are 320 participants per group, the mean of each group is estimated very accurately, but with 10 per group, it is very unreliable, and in some of our studies the mean of the blue group is lower than the mean of the pink group, which is opposite to what is true in the population. 


```{r makerNdata,echo=FALSE,include=FALSE}
#Plot taken from Law of Small numbers paper: needs formatting etc - possibly redoing in ggplot. For now, I have just read in the saved version. 
nsub <- 100 #set to size of largest group 
numrange<-c(10,20,40,80,160,320)
nset <- length(numrange)
sampledat<-data.frame(matrix(NA,nrow=nsub,ncol=nset)) #dummy frame to hold simulated data on nsub cases and nset columns
#prepare plot

ES<-.3

Nsample<-50

for (n in numrange){
  sampledat <- data.frame(matrix(NA,Nsample*2,3))
  colnames(sampledat)<-c('N','mean','ES')
  sampledat$N<-n
  thisrow<-0
  for(es in c(0,ES)){
    for (ns in 1:Nsample){
      thisrow<-thisrow+1
      sampledat$mean[thisrow] <- mean(rnorm(n,es,1))
      sampledat$ES[thisrow]<-es
    }
  }
  
  if(n==numrange[1]){ alldat<-sampledat}
  if(n>numrange[1]){alldat<-rbind(alldat,sampledat)} #bolt on next sample size in long format
}


#specify pink colour for control group, and blue for experimental group
alldat$colour <- 'deeppink'
w <- which(alldat$ES==ES)
alldat$colour[w]<-'blue'

beeswarm(alldat$mean~alldat$N,pwcol=alldat$colour,cex=.7,pch=16,xlab='Sample size',ylab='Observed effect size',ylim=c(-.7,.9))
abline(h=ES,lty=3,lwd=2,col='blue')
abline(h=0,lty=3,lwd=2,col='deeppink')
text(.6,-.62,'Power:',cex=.8)
for (samplesize in 1:6){
  mypower <-  power.t.test(n = numrange[samplesize], delta = ES, sd = 1, sig.level = 0.05,
                           type = c("two.sample"),
                           alternative = c("one.sided"))
  text((samplesize),-.7,round(mypower$power,3),cex=.8)
}

```

![Figure y. Simulated mean scores from samples of varying size, drawn from populations with either a null effect (pink) or a true effect size, Cohen's _d_, of `r ES` (blue). Power (discussed below) is the probability of obtaining p < .05 on a one-tailed t-test comparing group means for each sample size. ](bees_ES0.3_Nall.jpg)  


### Understanding p-values

When we do an intervention study we want to find out whether a given intervention works. The examples above show that it is not enough just to show that the mean outcome for an intervention group is better than for a control group. We need to take the variation around the means into account. 

Most studies use an approach known as Null Hypothesis Significance Testing, which gives us a rather roundabout answer to the question.  Typically, findings are evaluated in terms of p-values, which tell us _what is the probability (p) that our result could have arisen by chance_. The lower the p-value, the less likely it is that our result would have occurred if there is no real intervention effect. 

P-values are very often misunderstood, and there are plenty of instances of wrong definitions being given even in textbooks. The p-value is _the probability of the observed data, if the null hypothesis is true_.  It does _not_ tell us the probability of the null hypothesis being true. And it tells us nothing about the alternative hypothesis, i.e., that the intervention works. 

An analogy might help here. Suppose you are a jet-setting traveller and you wake up one morning confused about where you are. You wonder if you are in Rio de Janiero - think of that as the null hypothesis. You look out of the window and it is snowing. You decide that it is very unlikely that you are in Rio. You reject the null hypothesis. But it's not clear where you are.  Of course, if you knew for sure that you were either in Reykjavík or Rio, then you could be pretty sure you were in Reykjavík. But suppose it was *not* snowing. This would not leave you much the wiser.  

A mistake often made when interpreting p-values is that people think it tells us something about the probability of a hypothesis being true. That is not the case. There are alternative Bayesian methods that can be used to judge the relatively likelihood of one hypothesis versus another, given some data, but they do not involve p-values. 

A low p-value allows us to reject the null hypothesis, but this does no more than indicate "something is probably going on here - it's not just random" - or, in the analogy above, "I'm probably not in Rio". 


<!---START CUSTOM BLOCK-->
There are many criticisms of the use of p-values in science, and a good case can be made for using alternative approaches, notably methods based on Bayes theorem. Our focus here is on Null Hypothesis Significance Testing purely because such a high proportion of studies in the literature use this approach, and it is important to understand p-values in order to evaluate the literature.  

One good reason for objecting to the use of p-values is that they are typically used to make a binary decision: we either accept or reject the null hypothesis, depending on whether the p-value is less than a certain value. In practice, evidence is graded, and it can be more meaningful to express results in terms of the amount of confidence in alternative interpretations, rather than as a single accept/reject cutoff.

links to good intros to Bayes?

<!---END CUSTOM BLOCK-->


In practice, p-values are typically used to divide results into "statistically significant" or "non-significant", depending on whether the p-value is low enough to reject the null hypothesis. We do not defend this practice, which can lead to an all-consuming focus on whether p-values are above or below a cutoff, rather than considering effect sizes and strength of evidence. However, it is important to appreciate how the cutoff approach leads to experimental results falling into 4 possible categories, as shown in figure x. 

  
```{r confusionMat, echo=FALSE, message=FALSE, warnings=FALSE,results='asis'}
options(kableExtra.html.bsTable = T)
mymat<-matrix(NA,nrow=2,ncol=3)

mymat[,1]<-c("Reject Null hypothesis","Do Not Reject Null Hypothesis")

mymat[,2]<-c("True Positive","False Negative \n (Type II error)")
mymat[,3]<-c("False Positive \n (Type I error)","True Negative")

mymat<-as.data.frame(mymat)
names(mymat)<-c("","Intervention effective","Intervention ineffective")

knitr::kable(mymat,escape = F, align = "c", booktabs = T, caption = 'Outcomes of hypothesis test') %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F) %>%
  column_spec(1:1, bold = T) %>%
  column_spec(1:3,width = "9em") %>%
  add_header_above(c(" ","Ground truth" = 2))
```

The Ground truth is the result that we would obtain if we were able to administer the intervention to the whole population - this is of course impossible, but we assume that there is some general truth that we are aiming to discover by running our study on a sample from the population. We can see that if the intervention really is effective, and the evidence leads us to reject the null hypothesis, we have a True Positive, and if the intervention is ineffective and we accept the null hypothesis, we have a True Negative. Our goal is to design our study so as to maximise the chances that our conclusion will be correct, but there two types of outcome that we can never avoid, but which we try to minimise, known as Type I and Type II errors. We will cover Type II errors first, as these depend crucially on sample size and effect size. 

### Type II error ($\beta$)

**A false negative occurs when the null hypothesis is accepted and a true effect is actually present.**

Suppose a researcher wants to test a well-established and frequently-replicated result: children who read more frequently obtain better exam results when they are tested on vocabulary. 20 families are split into two groups; in the first group, parents are encouraged to read with their child each night for a month, whereas in the other group no such encouragement is given. The study is run, but when children's vocabulary results are compared, the statistical test results in a non-significant p-value, greater than .05.

The researcher is confused as she knows there is research evidence to indicate that an effect should be present. Unfortunately, she has failed to take into account the fact that the effect is fairly small, and to show it convincingly she would need a much larger sample size.

<!---I took out binary classifier as not so relevant for intervention research-->

## Statistical power

Statistical power is the probability that a study will be able to show a significant difference when there is a true effect.  If we take $\beta$ as the proportion of nonsignificant results that are false negatives, then power is 1-$\beta$ - expressed either as a proportion or a percentage. So if I say my study has 50% power, that means that, based on my expectation of the effect size, if I were to run the study 10 times, on 5 occasions I would fail to obtain a significant difference.  Power depends on:

- Sample size
- "Significance criterion", also known as the Type 1 error rate ($\alpha$)
- Size of the effect of interest 

We repeat here Figure x, which shows simulated data for different sample sizes when effect size is 0.3, plus the same scenario for effect sizes of .2 and .5 [not yet done]. One can see how power is lower for small sample size, especially when effect size is small. 

<!---To do: maybe make a figure with plots stacked above each other for the different effect sizes ?-->

![Figure . Simulated mean scores from samples of varying size, drawn from populations with either a null effect (pink) or a true effect size, Cohen's _d_, of `r ES` (blue). Power (discussed below) is the probability of obtaining p < .05 on a one-tailed t-test comparing group means for each sample size. ](bees_ES0.3_Nall.jpg)


We can also show how power is affected by changing the $\alpha$ level - this affects how large a difference we need to see between groups before we reject the null hypothesis. When $\alpha$ is more extreme, we will make fewer false positive errors (see chapter 10), but we will make more false negatives. 

Figure x illustrates this in a simple example using a z-test, which is a one sample location test that simply assesses whether a sample of some quantity of interest has the same sample mean as the population. Suppose we have a single child's change score on a reading test, and we want to know whether they had an effective reading intervention. Figure x below shows the distribution of scores for children who had no intervention in the top: this is a density plot, showing how common different scores are (frequency on the y-axis), with individual scores on the x-axis. The shape follows a normal distribution: most scores are in the middle, with higher or lower scores being less common, and the mean is zero and standard deviation is one. The null hypothesis is that the child's score comes from this distribution.

The lower figure is the distribution for children who had the reading intervention. The intervention had a large effect (Cohen's _d_ of one), and so the whole distribution is shifted over to the right. We're going to use a one-sided z-test, because we know that the child's score will either come from the null distribution, or from one with a higher mean. We decide to use the conventional level of $\alpha$ of .05. The vertical dotted line is therefore placed at a point where 5% of the upper distribution (the red area) is to the right of the line, and 95% (the white area) to the left. This point can be worked out from knowledge of the normal distribution and corresponds to a score on the x-axis of 1.65. Our rule is that if the child's score is greater than 1.65, we reject the null hypothesis. If it is less than 1.65, we can't reject the null hypothesis. This does not mean that the child definitely came from the non-intervention group - just that the evidence is not sufficient to rule that out. Regardless of the effect size, if  $\alpha$ level is set to .05, we wrongly reject the null hypothesis on only 1 in 20 experiments. 

But now look down to the lower distribution. False negatives are represented by the blue area to the left of the dotted line: cases where we fail to reject the null hypothesis, but the score really came from the group with intervention. The power of the study corresponds to the yellow region, where we have a score greater than 1.65 and the data really come from the intervention condition. But notice that power is extremely low: the blue area is much bigger than the yellow area. We are much more likely to wrongly retain the null hypothesis than to correctly reject it - even though we have a pretty large effect size.  So while the false positive rate is kept constant by selection of $\alpha$ of .05, the false negative result is not. 

If we wanted to make the false negative (type II error) rate much lower, we could adopt a less stringent alpha level, e.g. we could move the vertical dotted line to the point where the x-axis was zero, so the yellow area becomes much bigger than the blue area.  But if we do that, we then would increase the type I (false positive) error rate to 50%!  

```{r densplot,echo=FALSE, message=FALSE, warnings=FALSE, fig.cap="Z test: statistical power, N=1"}
#https://stats.stackexchange.com/questions/14140/how-to-best-display-graphically-type-ii-beta-error-power-and-sample-size

#TeachingDemos package in R for inspiration.

library(magrittr)
library(ggplot2)
library(ggpubr)

mha <- 1  # mean under the alternative
z <- qnorm(1-0.05,0,1/sqrt(1))   
n <- 1
x <- seq(-3, 4, length=1000)
dh0 <- dnorm(x, 0, 1/sqrt(n))
dh1 <- dnorm(x, mha, 1/sqrt(n))
mydata<-data.frame(x,dh0,dh1) %>% mutate(area = x >= z)


p1<-ggplot(data = mydata, aes(x = x, ymin = 0, ymax = dh0, fill = area)) +
  geom_ribbon(alpha=0.5) +
  geom_line(aes(y = dh0)) +
  geom_vline(xintercept = z, linetype = 'dashed') +
  theme_bw()+ylab("") +theme(legend.title = element_blank(),legend.text = element_text(c("Alpha","")))+ scale_fill_manual(labels = c("","Alpha"),values=c("white","red"))+
  annotate(geom = 'text', x = z+0.2, y = 0.15, colour="red", label = 'Type I Error\n (Alpha)', hjust = -0.2)+annotate(geom = 'text', x = -2, y = 0.3, label = 'Mean under\n Null, H0 = 0')+xlab("")+theme(legend.position = "none")+ggtitle("NULL DISTRIBUTION")

p2<-ggplot(data = mydata, aes(x = x, ymin = 0, ymax = dh1, fill = area)) +
  geom_ribbon(alpha=0.5) +
  geom_line(aes(y = dh1)) +
  geom_vline(xintercept = z, linetype = 'dashed') +
  theme_bw()+ylab("") +theme(legend.title = element_blank(),legend.text = element_text(c("Power","Type II (Beta)")))+ scale_fill_manual(labels = c("Type II (Beta)","Power"),values=c("Blue","yellow"))+
  annotate(geom = 'text', x = -0.25, y = 0.1, label = 'Type II \nError (Beta)', hjust = -0.1) +
  annotate(geom = 'text', x = z+0.2, y = 0.1, label = 'Power\n (1-Beta)', hjust = -0.01)+
  annotate(geom = 'text', x = -2, y = 0.3, label = 'Mean under \nalternative, H1 = 1')+xlab("")+theme(legend.position = "none")+ggtitle("ALTERNATIVE DISTRIBUTION")

ggarrange(p1, p2, ncol = 1, nrow = 2)


```

The second figure \@ref(fig:densplot2) presents the same one-sided z test but here the sample size has increased to 10. A key point is that the density plot here does _not_ show the distribution of scores from individual children in any one sample; it is the distribution of _means_ that we would see if we repeatedly took samples of a given size. So if we had a population of 10,000 children, and just kept taking samples of 10 children, each of those samples would have a mean, and it is these that are plotted here.  We should notice that two things have appeared to change. First, we see a greater distinction between the two distributions. Second, we see that the critical $z$ value (vertical dashed line) has changed location. The distributions have not changed their location (the peak of each bell shaped curve is the same), but the spread of each distribution has shrunk as a result of the increased sample size, because the precision of the estimate of the mean improves with a larger sample. 



```{r densplot2,echo=FALSE, message=FALSE, warnings=FALSE, fig.cap="Z test: statistical power, N=10"}
#https://stats.stackexchange.com/questions/14140/how-to-best-display-graphically-type-ii-beta-error-power-and-sample-size

#TeachingDemos package in R for inspiration.

library(magrittr)
library(ggplot2)
library(ggpubr)

mha <- 1  # mean under the alternative
n <- 10
z <- qnorm(1-0.05,0,1/sqrt(n))   

x <- seq(-3, 4, length=1000)
dh0 <- dnorm(x, 0, 1/sqrt(n))
dh1 <- dnorm(x, mha, 1/sqrt(n))
mydata<-data.frame(x,dh0,dh1) %>% mutate(area = x >= z)


p1<-ggplot(data = mydata, aes(x = x, ymin = 0, ymax = dh0, fill = area)) +
  geom_ribbon(alpha=0.5) +
  geom_line(aes(y = dh0)) +
  geom_vline(xintercept = z, linetype = 'dashed') +
  theme_bw()+ylab("") +theme(legend.title = element_blank(),legend.text = element_text(c("Alpha","")))+ scale_fill_manual(labels = c("","Alpha"),values=c("white","red"))+
  annotate(geom = 'text', x = z+0.2, y = 0.25, colour="red", label = 'Type I Error\n (Alpha)', hjust = -0.2)+annotate(geom = 'text', x = -1, y = 1, label = 'Mean under\n Null, H0 = 0')+xlab("")+theme(legend.position = "none")+ggtitle("NULL DISTRIBUTION")

p2<-ggplot(data = mydata, aes(x = x, ymin = 0, ymax = dh1, fill = area)) +
  geom_ribbon(alpha=0.5) +
  geom_line(aes(y = dh1)) +
  geom_vline(xintercept = z, linetype = 'dashed') +
  theme_bw()+ylab("") +theme(legend.title = element_blank(),legend.text = element_text(c("Power","Type II (Beta)")))+ scale_fill_manual(labels = c("Type II (Beta)","Power"),values=c("Blue","yellow"))+
  annotate(geom = 'text', x = -0.5, y = 0.5, label = 'Type II \nError (Beta)', hjust = -0.1) +
  annotate(geom = 'text', x = z+0.2, y = 0.5, label = 'Power\n (1-Beta)', hjust = -0.01)+
  annotate(geom = 'text', x = -1, y = 1, label = 'Mean under \nalternative, H1 = 1')+xlab("")+theme(legend.position = "none")+ggtitle("ALTERNATIVE DISTRIBUTION")

ggarrange(p1, p2, ncol = 1, nrow = 2)


```

The shaded areas on the density plots directly relate to the concepts outlined above: power, type I, and type II errors. When the sample size increases, the standard error (SE) and $z$ score both reduce.  We notice that the type I error rate (area in red) is proportionally the same at 5% and the effect size remains fixed, but we see a change in the only two remaining quantities, power and type II rate. This is because these quantities are linked. The area under the density curve must always remain at 1, so proportionally, we can calculate the power as 1-(type II rate, $\beta$) and vice versa. We can visually see this in both figures by looking at the specified areas for the alternative distribution.  
  
If you are finding this all quite confusing, don't worry. This is complicated and even those who have statistical training can find it challenging. The most important points to take away from these examples are that:
- Statistical power depends on the sample size and the effect size, as well as the level of $\alpha$  
- With small samples, studies often have low power, meaning that even if there is a real effect, there may be little chance of detecting it. 
- A p-value greater than .05 does not mean the null hypothesis is true.

It is therefore important to think about power when designing a study, or you may end up concluding an intervention is ineffective, when in fact it has a small effect that your study is underpowered to detect.  


<!---START CUSTOM BLOCK-->
**Standard error of the mean**  

It can be challenging to get an intuitive understanding of power, because the computations needed to calculate it are not straightforward. A key statistic is the standard error of the mean, also known as the SEM, usually shortened to standard error (SE). This can be thought of as an index of the variability of an estimate of the mean from a sample. If you imagine taking numerous samples from a population, and estimating the mean from each one, you will end up with a distribution of means, similar to those shown in Figure [fig above with pink and blue beeswarms]. As shown in that figure, these estimates of the mean are much more variable for small than for large samples.  The SE is the standard deviation of the estimates of the mean, and it is crucially dependent on the sample size. 
This follows form the formula for the SE, which is:

$SE = \frac{SD}{\sqrt(n)}$, where $n$ is the sample size, and $SD$ is the standard deviation of scores in the population.

The $z$ value, which is used to derive a p-value, uses the SE as a denominator, and so will also be influenced by sample size. The $z$ score is defined as follows,

$z =\frac{M-\mu}{SE}$, where $M$ is the sample mean score, $\mu$ is the population mean, and $SE$ is as defined above.

The lower the SE, the more precise the measurement, and the higher the power of the statistical test. Note that the value entered into these equations is the square root of N. It follows that improvement in precision from adding extra partipicants to a study is greatest at small sample sizes: as shown in figure zz, the SE is approximately halved in increasing the sample size from 10 to 40, whereas changes are much smaller in going from 110 to 140 participants. 

```{r demoSE, echo=F, include=T} 
myseq <- seq(10,200,10)
plot(myseq,1/sqrt(myseq),type='b',ylab='SE',xlab='N')

```
<!---END CUSTOM BLOCK-->

Typically, clinical trials in medicine are designed to achieve 80% statistical power and, depending on the statistical analysis strategy, will employ some method to control type I error rate (traditionally $\alpha=0.05$). With $\alpha$ fixed, power depends on effect size and sample size. 

So the first question is how do we select an effect size. This is quite a thorny issue. In the past, it was common just to base anticipated effect sizes on those obtained in previous studies, but these can be overestimates because of publication bias (see chapter 19). An alternative approach is to consider what is the smallest effect size that would be of interest: for instance, if you have a vocabulary intervention on which children start with a mean score of 20 words (SD of 10) would you be interested in an average improvement on an outcome test of half a word, or would you think the intervention would only be worthwhile if children improved on average by 4 or more words? Lakens (@lakens2021) has a useful primer on how to justify a sample size.  

Once a suitable effect size is established, then it is possible to compute power for different effect sizes, to work out how big a sample would be needed to attain the desired statistical power, typically set to 80% or more. 

Researchers are often shocked when they first do a power analysis, to find that sample sizes that are conventionally used in their field are not adequately powered. Even more depressing, a power analysis may tell you that you would need an unfeasibly large sample in order to show an effect of interest. Researchers who start out planning a study with 20 individuals per group may be discouraged to find that they need 80 per group to do a meaningful study. This is a common predicament, but there are some ways forward:

- You may be able to improve the effect size of your outcome measure by careful consideration of reliability of the outcome measure. Remember, effect size is the difference in means divided by the standard deviation: if you can reduce the standard deviation by minimising random noise from your outcome measure, you will improve the effect size.  
- It is worth consulting with a statistician about the optimal research design. Some designs are more efficient than others for showing effects of interest. 
- If the necessary sample size is too large for you to achieve, it may be worth considering forming a consortium by joining forces with other researchers.  This is covered further in chapter 19. 

EXERCISES
(look at power in published literature?)