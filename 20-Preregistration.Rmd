#	Pre-registration and Registered Reports

## Trial registration  
The basic idea of study registration is that the researcher declares in advance key details of the study - effectively a full protocol that explains the research question and the methods that will be used to address it. Crucially, this should specify the primary outcome and the method of analysis, without leaving the researcher any wiggle room to tweak results to make them look more favourable. A study record in a trial registry should be public and time-stamped, and completed before any results have been collected. Since the Food and Drug Administration Modernization Act of 1997 first required FDA-regulated trials to be deposited in a registry, other areas of medicine have followed, with a general registry, ClinicalTrials.gov, being established in 2000. Registration of clinical trials has become widely adopted, and is now required if one wants to publish trial results in a reputable journal [@deangelis2004].

Trial registration serves several functions, but perhaps the most important one is that it makes research studies visible, regardless of whether they obtain positive results. In chapter 19, we showed results from a study that was able to document publication bias [@devries2019] precisely because studies in this area were registered. Without trial registration, we would have no way of telling that the unpublished studies had ever existed. 

A second important function of trial registration is that it allows us to see whether researchers did what they planned to do. Of course, â€œThe best laid schemes o' mice an' men / Gang aft a-gley", as Robert Burns put it. It may turn out impossible to recruit all the participants one hoped for. An outcome variable may turn out to be unsuitable for analysis. A new analytic method may come along which is much more appropriate for the study. The purpose of registration is not to put the study in a straightjacket, but rather to make it transparent when there are departures from the protocol. In particular, given the propensity of researchers to (illegitimately) change their hypothesis on the basis of seeing the data, registration provides an important way of keeping us honest. In practice, registration does not always achieve this: @goldacre2019 identified 76 trials published in a six-week period in one of five journals: New England Journal of Medicine, The Lancet, Journal of the American Medical Association, British Medical Journal, and Annals of Internal Medicine. These are all high-impact journals that officially endorse Consolidated Standards of Reporting Trials (CONSORT), which specify that pre-specified primary outcomes should be reported. Not only did Goldacre et al find high rates of outcome-switching in these trial reports; they also found that some of the journals were reluctant to publish a letter that drew attention to the mismatch, with qualitative analysis demonstrating "extensive misunderstandings among journal editors about correct outcome reporting". 

In psychology, a move towards registration of studies has been largely prompted by concerns about the high rates of p-hacking and HARKing in this literature [@simmons2011](see Chapter 11), and the focus is less on clinical trials than on basic observational or experimental studies. The term "pre-registration" has been adopted to cover this situation. For psychologists, the Open Science Framework has become the most popular repository for pre-registrations, allowing researchers to deposit a time-stamped protocol, which can be embargoed for a period of time if it is desired to keep this information private [@hardwicke2021]. Examples of pre-registration templates are available [https://cos.io/rr/](https://cos.io/rr/)

#Registered Reports  
Michael Mahoney, whose book was mentioned in Chapter 19, provided an early demonstration of publication bias with his little study of journal reviewers [@mahoney1976]. Having found that reviewers are far too readily swayed by a paper's results, he recommended: _Manuscripts should be evaluated solely on the basis of their relevance and their methodology. Given that they ask an important question in an experimentally meaningful way, they should be published - regardless of their results._ (p. 105).

This is exactly the publishing model that was introduced by Chris Chambers, in his editorial role at the journal Cortex, in 2013. This initiative was heralded by an open letter in the Guardian https://www.theguardian.com/science/blog/2013/jun/05/trust-in-science-study-pre-registration with more than 80 signatories. 

Figure x shows how registered reports differ from the traditional publishing model. The registered report is a specific type of journal article that embraces pre-registration as one element of the process; crucially, peer review occurs before data is collected. The decision whether or not to accept the article for publication is based on the introduction, methods and analysis plan, before results are collected. At this point reviewers cannot be influenced by the results, as they have not yet been collected. There is a further check on the article after data collection, but the "in principle" acceptance cannot be overturned by reviewers coming up with new demands at this point: provided the authors have done what they said they would do, the acceptance should hold. This unique format turns the traditional publishing paradigm on its head. Peer review can be viewed less as "here's what you got wrong and should have done" and more like a helpful co-author that gives feedback at a stage in the project that things can be adapted to improve the study. At the same time, methodological quality of registered reports tends to be high because no editor or reviewer wants to commit to publish a study that is poorly conducted, underpowered or unlikely to give a clear answer to an interesting question. Registered reports are required to specify clear hypotheses, give specification of an analysis plan to test these, justify the sample size, and document how issues such as outlier exclusion and participant selection criteria will be handled.

Figure X shows [see @Chambers_2018] shows the workflow of the typical research article and highlights the points at which reviews are conducted. A stage 1 review occurs once the study has been designed and statistical analysis plan devised. After this review, an in principle acceptance can be offered which crucially is results free, i.e. publication is guaranteed regardless of the result and provided that the authors perform the analysis as planned or transparently document any deviations from the planned design. A second stage review is then conducted but this is much lighter touch and only check whether the authors did as they had said they would and any claims have not been inflated. 

![](RR_compare_regularpublishing.jpg)
The terminology in this area can be rather confusing, and it is important to distinguish between 
 pre-registration, as described in the previous section (which in the clinical trials literature is simply referred to as "trial registration") and registered reports, which are considerably more specific, as shown in this Venn diagram [@Chambers_2018][Figure 1]. In general, registered reports have the same characteristics as pre-registered studies, though some journals will accept registered reports without requiring that the pre-registration is made public - hence the lack of complete overlap in the Venn diagram.
 
 <!---Paul - I do wonder about the Venn diagram. I think the way the RR circle hangs out of the side of the prereg circle is actually quite confusing, and maybe not necessary in this specific context? I have tried to describe it in words, but wonder if we need it-->

![](Chambers venn diagram.jpg)

The more stringent requirements for a registered report, versus standard pre-registration, mean that this publication model can counteract four major sources of bias in scientific publications - referred by @bishop2019 as the four horsemen of the reproducibility apocalypse, namely:

- Publication bias. By basing reviews on introduction and methods only, it is no longer possible for knowledge of results to influence publication decisions. As @mahoney1976 put it, it allows us to : _place our trust in good questions rather than cheap answers._  

- Low power. No journal editor wants to publish an ambiguous null result that could just be the consequence of low statistical power (Chapter 10). However, in an adequately powered intervention study, a null result is important and informative for telling us what does not work. Registered reports require authors to justify their sample size, minimising the likelihood of type II errors.  

 - P-hacking. Pre-specification of the analysis plan makes transparent the distinction between pre-planned hypothesis-testing analyses, and post hoc exploraration of the data. Note that exploratory analyses are not precluded in a registered report, but they are reported separately, on the grounds that statistical inferences need to be handled differently in this case (see Chapter 11).  

 - HARKing. Because hypotheses are specified before the data are collected, it is no longer possible to use the same data to both generate and test hypotheses. HARKing is so common as to be normative in many fields, but it generates a high rate of false positives when a hypothesis that is only specified after seeing the data is presented as if it was the primary motivation for a study. 
 
Registered reports are becoming increasingly popular in psychology, and are beginning to be adopted in other fields, but many journal editors have resisted adopting this format. In part this is because any novel system requires extra work, and in part because of other concerns - e.g. that this might lead to less interesting work being published in the journal. Answers to frequently asked questions about registered reports can be found on Open Science Framework https://www.cos.io/initiatives/registered-reports?_ga=2.82702869.289676806.1627987440-1355006079.1608014416. As might be gathered from this account, we are enthusiastic advocates of this approach, and have co-authored several registered reports ourselves. 


