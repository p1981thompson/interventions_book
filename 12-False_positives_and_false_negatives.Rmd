---
output:
  pdf_document: default
  html_document: default
---
```{r packages12}
list_of_packages<-c("tidyverse","kableExtra","knitr")
new.packages <- list_of_packages[!(list_of_packages %in% installed.packages()[,"Package"])]
if(length(new.packages))install.packages(new.packages,dependencies = TRUE)

library(tidyverse)
library(kableExtra)
library(knitr)
```

# False positives and false negatives: are we missing true effects?

The aim of many research studies is to investigate whether there is evidence for a specific hypothesis. We typically want to test this hypothesis in a robust way that gives us a clear indication of evidence for or against. Study design is paramount in ensuring that we can trust our results and have confidence in any inferences drawn. The implications of poor study design are not easily visible as many statistical tests will generate results regardless of whether certain test assumptions are met, or if we are using a small sample. This failure to meet certain assumptions or using an insufficient sample size can lead to an increase in false positives, also known as a Type I errors ($\alpha$), or an increase in false negatives, also known as Type II errors ($\beta$). Some more formal definitions with examples will help us to clarify these concepts.

### Type I error ($\alpha$)

**A false positive occurs when the Null hypothesis is rejected and a true effect is not actually present.** Typically, the type I error rate is controlled by setting the "significance criterion" at 5% or sometimes more conservatively 1%. In real terms, this would indicate that if $\alpha=0.05$, we would expect 1 in 20 results to have occurred by chance. 

For example, Researcher X wants to test the hypothesis that eating carrots improves children's IQ by 2 points. The researcher assigns 6 children to two groups, one group eat carrots and the other group eat their usual snack. The data is collected and a significant finding is found (Null hypothesis that carrots do not cause a 2 point increase in IQ is rejected). The researcher is jubilant that he has discovered an interesting result for publication. 

His sceptical colleague decides to check this result and decides to try to replicate the finding. The colleague re-runs the study with a new sample of children, but does not alter any other details of the study. Unfortunately, the colleague does not replicate the finding. The first result is likely to be a false positive for a number of reasons including sampling variability in small samples, small effect size, or even just poor quality data.    

### Type II error ($\beta$)

**A false negative occurs when the Null hypothesis is accepted and a true effect is actually present.**

Now consider a second example, where the sceptical colleague wants to test a well-established and frequently-replicated result. she wants to confirm whether on average children who read more frequently obtain better exam results when they are tested on vocabulary. Once again, 6 different children are split into two groups, one group reads with a parent each night for a month and the other group continues with their usual routine. The study is run and the Null hypothesis is accepted as the statistical test results in a non-significant p-value (P>=0.05, in this case). 

The sceptical colleague is confused as she cannot replicate a known effect, but there is overwhelming evidence to inidicate that an effect should be present. We look to the study design and oncemore, we see that the sample size was small and sampling variability will have played some role in causing a type II error. 

Table \ref{confusion_mat} presents a contingency table of the potential combinations of test or predicted condition and true condition. This is a common representation of the classifications of power, false positive, false negative, and true negative for a binary test classifier.  


```{r confusion_mat, echo=FALSE, message=FALSE, warnings=FALSE,results='asis',fig.cap=""}
mymat<-matrix(NA,nrow=2,ncol=4)

mymat[,1]<-rep("Predicted",2)
mymat[,2]<-c("Predicted Positive","Predicted Negative")
mymat[,3]<-c("True Positive \n POWER","False Negative \n Type II error")
mymat[,4]<-c("False Positive \n Type I error","True Negative")

mymat<-as.data.frame(mymat)
names(mymat)<-c("","","Condition positive","Condition negative")

kable(mymat, align = "c") %>%
  kable_styling(c("striped", "bordered"),full_width = F) %>%
  column_spec(1:2, bold = T) %>%
  column_spec(1:4,width = "9em") %>%
  collapse_rows(columns = 1, valign = "middle")%>%add_header_above(c(" "," ","True" = 2))
```

## Statistical power

As we have seen in our earlier examples, a frequent concern in study design refers to sample size and it's related concept of statistical power. Power can be defined as the probability that some statistical test will reject the Null hypothesis given that an alternative hypothesis of interest is true. 

Consider the following example, we want to investigate whether there is a significant difference in their mean score between the two groups, perhaps an intervention and a control group. The indivduals are randomly assigned to each group and for simplicity we assume that there are no additional differences between the groups. The intervention is administered and we collect data on all individuals before running our statistical analysis. The power is then a function of several quantities:

- Sample size
- "Significance criterion", also known as the Type 1 error rate ($\alpha$)
- Size of the effect of interest 
- Type II error ($\beta$, note that Power=1-$\beta$)

We can show the relation between these quantities of interest visually through an example using the z-test which is a simple one sample location test. The idea of this test is to assess whether a sample of some quantity of interest has the same sample mean as the population of the same quantity. In the figures below we will use a one-sided z test, which indicates that we have a direction specific hypothesis. We could change this to a two sided test which is more common in practice due to the fact that it is more realistic to test for a difference regardless of direction. 

The first figure shows the case for a single individual, n=1. We have set the standard deviation of both Null and Alternative distributions to equal one, and their means to zero and one respectively. When we have a sample size of 1 individual, we see that a small area is defined for power, indicating lower power and higher Type II error.   

```{r densplot,echo=FALSE, message=FALSE, warnings=FALSE}
#https://stats.stackexchange.com/questions/14140/how-to-best-display-graphically-type-ii-beta-error-power-and-sample-size

#TeachingDemos package in R for inspiration.

library(magrittr)
library(ggplot2)
library(ggpubr)

mha <- 1  # mean under the alternative
z <- qnorm(1-0.05,0,1/sqrt(1))   
n <- 1
x <- seq(-3, 4, length=1000)
dh0 <- dnorm(x, 0, 1/sqrt(n))
dh1 <- dnorm(x, mha, 1/sqrt(n))
mydata<-data.frame(x,dh0,dh1) %>% mutate(area = x >= z)


p1<-ggplot(data = mydata, aes(x = x, ymin = 0, ymax = dh0, fill = area)) +
  geom_ribbon(alpha=0.5) +
  geom_line(aes(y = dh0)) +
  geom_vline(xintercept = z, linetype = 'dashed') +
  theme_bw()+ylab("") +theme(legend.title = element_blank(),legend.text = element_text(c("Alpha","")))+ scale_fill_manual(labels = c("","Alpha"),values=c("white","red"))+
  annotate(geom = 'text', x = z+0.2, y = 0.15, colour="red", label = 'Type I Error\n (Alpha)', hjust = -0.2)+annotate(geom = 'text', x = -2, y = 0.3, label = 'Mean under\n Null, H0 = 0')+xlab("")+theme(legend.position = "none")+ggtitle("NULL DISTRIBUTION")

p2<-ggplot(data = mydata, aes(x = x, ymin = 0, ymax = dh1, fill = area)) +
  geom_ribbon(alpha=0.5) +
  geom_line(aes(y = dh1)) +
  geom_vline(xintercept = z, linetype = 'dashed') +
  theme_bw()+ylab("") +theme(legend.title = element_blank(),legend.text = element_text(c("Power","Type II (Beta)")))+ scale_fill_manual(labels = c("Type II (Beta)","Power"),values=c("Blue","yellow"))+
  annotate(geom = 'text', x = -0.25, y = 0.1, label = 'Type II \nError (Beta)', hjust = -0.1) +
  annotate(geom = 'text', x = z+0.2, y = 0.1, label = 'Power\n (1-Beta)', hjust = -0.01)+
  annotate(geom = 'text', x = -2, y = 0.3, label = 'Mean under \nalternative, H1 = 1')+xlab("")+theme(legend.position = "none")+ggtitle("ALTERNATIVE DISTRIBUTION")

ggarrange(p1, p2, ncol = 1, nrow = 2)


```

The second figure presents the same one-sided z test but here the sample size has increased. We should notice that two things have appeared to change. Firstly, we see a greater distinction between the two distributions. Secondly, we see that the $z$ value (vertical dashed line) has changed location. The distributions have not changed their location but the spread of each distribution as shrunk as a result of the sample size as the spread is directly proportional to the sample size. The reason for this change lies in the formula for the standard error and $Z$ score test statistic, 

$SE = \frac{SD}{\sqrt(n)}$

where $n$ is the sample size, and $SD$ is the standard deviation.

The $z$ value or score is also proportional to the sample size, so we see that this value changes accordling. The $z$ score is defined as follows,

$z =\frac{M-\mu}{SE}$

where $M$ is the sample mean score, $\mu$ is the population mean, and $SE$ is as defined above.

The shaded areas on the density plots directly relates to the concepts oulined above: power, type I, and type II errors. When the sample size increases, the standard error (SE) and $z$ score both reduce. The only input parameter that has changed between the two figures is that the power has increased. We notice that the type I error rate (area in red) is proportionally the same at 5% and the effect size remains fixed, so we see a change in the only two remaining quantities, power and type II rate. This is because these quantities are linked. The area under the density curve must always remain at 1, so proportionally, we can calculate the power as 1-(type II rate, $\beta$) and vice versa. We can visually see this in both figures by looking at the specified areas for the alternative distribution.  
  

```{r densplot2,echo=FALSE, message=FALSE, warnings=FALSE}
#https://stats.stackexchange.com/questions/14140/how-to-best-display-graphically-type-ii-beta-error-power-and-sample-size

#TeachingDemos package in R for inspiration.

library(magrittr)
library(ggplot2)
library(ggpubr)

mha <- 1  # mean under the alternative
n <- 10
z <- qnorm(1-0.05,0,1/sqrt(n))   

x <- seq(-3, 4, length=1000)
dh0 <- dnorm(x, 0, 1/sqrt(n))
dh1 <- dnorm(x, mha, 1/sqrt(n))
mydata<-data.frame(x,dh0,dh1) %>% mutate(area = x >= z)


p1<-ggplot(data = mydata, aes(x = x, ymin = 0, ymax = dh0, fill = area)) +
  geom_ribbon(alpha=0.5) +
  geom_line(aes(y = dh0)) +
  geom_vline(xintercept = z, linetype = 'dashed') +
  theme_bw()+ylab("") +theme(legend.title = element_blank(),legend.text = element_text(c("Alpha","")))+ scale_fill_manual(labels = c("","Alpha"),values=c("white","red"))+
  annotate(geom = 'text', x = z+0.2, y = 0.25, colour="red", label = 'Type I Error\n (Alpha)', hjust = -0.2)+annotate(geom = 'text', x = -1, y = 1, label = 'Mean under\n Null, H0 = 0')+xlab("")+theme(legend.position = "none")+ggtitle("NULL DISTRIBUTION")

p2<-ggplot(data = mydata, aes(x = x, ymin = 0, ymax = dh1, fill = area)) +
  geom_ribbon(alpha=0.5) +
  geom_line(aes(y = dh1)) +
  geom_vline(xintercept = z, linetype = 'dashed') +
  theme_bw()+ylab("") +theme(legend.title = element_blank(),legend.text = element_text(c("Power","Type II (Beta)")))+ scale_fill_manual(labels = c("Type II (Beta)","Power"),values=c("Blue","yellow"))+
  annotate(geom = 'text', x = -0.5, y = 0.5, label = 'Type II \nError (Beta)', hjust = -0.1) +
  annotate(geom = 'text', x = z+0.2, y = 0.5, label = 'Power\n (1-Beta)', hjust = -0.01)+
  annotate(geom = 'text', x = -1, y = 1, label = 'Mean under \nalternative, H1 = 1')+xlab("")+theme(legend.position = "none")+ggtitle("ALTERNATIVE DISTRIBUTION")

ggarrange(p1, p2, ncol = 1, nrow = 2)


```

An important point to note is that each of the following elements: power, sample size, effect size, and type I and II errors are intertwinned with each other. Hence, if one element is changed this has some effect on the other quantities. It also gives an insight on how we can use this knowledge to design our studies to minimise type I and II errors and increase statistical power. 

Typically, clinical trials in medicine will design studies to achieve 80% statistical power and depending on the statistical analysis strategy, will employ some method to the control type I error rate or use a more conservative rate (traditionally in NHST $\alpha=0.05$). Fixing these two quantities leaves us with a smaller number of elements to vary, but ususally we are designing to answer a particular scienctific question which will also provide theoretical constraints. More specifically, an effect size of interest will be dictated ideally from a metaanalysed estimate of the effect size or deisnged to the smallest effect size of interest that is practically or clinically meaningful. This leaves us with sample size as our only remaining element. 

A power calculation will vary sample sizes for a fixed statistical analysis, type I error rate, and effect size of interest. Sample size is usually most influenced by the size of effect and researcher constraints, i.e. funding or time. A large effect size will require fewer unique observations and conversely, smaller effect sizes require very large samples. The logic behind this can be seen in figure \ref{densplot2}, if we increase the effect size, the central tendencies (mean) of each distribution diverge, so the overlap of the distributions becomes less provided that the standard errors remains unchanged. 

## Multiple hypothesis testing

Even when we have a well designed and well powered study, it we collect mulitple outcome variables or if we are applying multiple tests to the same data, then the outcomes are unlikely to be completely independent, so we increase our change of finding a false positive. Returning to the idea that we have a type I error rate for a single test at 5%, then suppose we apply $n$ tests to our data. We increase our false positive rate ddramatically, as the new significance level is given by $1-(1-\alpha)^{n}$. If we assumed that we have performed 10 tests (n=10), we have increased our chance of a obtaining a false postive to approximately 40%.

There are many different ways to adjust for the multiple testing in practice. We shall discuss some of the most commonly applied.

### Bonferroni Correction

### Benjamini and Hochberg

### False Discovery rate 

### Permutation methods



