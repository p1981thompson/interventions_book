#	Single case designs
```{r packages15, message=F,warning=F,echo=FALSE}
list_of_packages<-c("tidyverse","kableExtra","knitr","ggpubr")
new.packages <- list_of_packages[!(list_of_packages %in% installed.packages()[,"Package"])]
if(length(new.packages))install.packages(new.packages,dependencies = TRUE)

library(tidyverse)
library(kableExtra)
library(knitr)
library(ggpubr)

```
## Single case (N-of-1) designs

The single case design, also known as N-of-1 trial, or small N design, is a commonly used intervention design in clinical psychology and neuropsychology, including aphasia therapy [@perdices2009]. The N-of-1 trial may be regarded as an extreme version of a _within-subjects_ design, where comparisons are made between conditions within a single person. Very often, a set of N-of-1 trials is combined into a *case series* (see below). It is important to note that an N-of-1 trial is not a simple case report, but rather a study that is designed and analysed in a way that controls as far as possible for the kind of unwanted influences on results described in chapters 2-4. 

Table x shows the contrast in logic between the standard RCT and N-of-1 designs.

```{r n-of-1-logic,echo=F,include=T}
Design <- c('RCT','N-of-1: ABA','N-of-1: Multiple baseline')
Participants <- c('+','-','-')
Time <- c('-','+','-')
Outcome <- c('-','-','+')
mydf <- data.frame(cbind(Design,Participants,Time,Outcome))
ftable<-flextable(mydf)
ftable <- set_caption(ftable, "How intervention is allocated in different study designs")
ftable <- autofit(ftable)
ftable

```

The first row of table 1 shows the design for a simple 2-arm RCT, where intervention is varied between subjects who are assessed on the same occasion and on the same outcome. The second row shows a version of the N-of-1 design where the invention is varied in a single subject at different time points. The third row shows the case where intervention is assessed by comparing different outcomes in the same subject on the same occasion. In practice, aspects of these designs can be combined - e.g. the cross-over design that we saw in chapter 16 compares an intervention across two time points and two groups of subjects. 

Whatever design is used, the key requirements are 

- (a) to minimise unwanted variance.  
- (b) to ensure that the effect we observe is as unbiased as possible.  

### Minimising unwanted variance
In the RCT, this is achieved by having a large enough sample of participants to distinguish variation associated with intervention from idiosyncratic differences between individuals, and by keeping other aspects of the trial, such as timing and outcomes, as constant as possible. 

With N-of-1 trials, we do not have to worry about variation associated with individual participant characteristics, but we do need to control for other sources of variation. The ABA design involves contrasting an outcome during periods of intervention versus periods of no intervention. For example, we may have a person who is using an auditory feedback device as an intervention for stuttering: we could observe their fluency during periods when the device is switched on and when it is switched off. We would want to repeat this comparison enough times to be certain any difference in fluency between conditions is robust, and not just due to the passage of time. Figure xx shows a fictitious example whee the difference between conditions is so striking that one does not need statistical tests to conclude that the intervention has an effect, but data are not always as clear as this. We discuss below statistical approaches to analysing data from such trials. 

```{r SingleCaseDplot, echo=F, message=F, warning=F, fig.width=8,fig.height=4, fig.cap="plot showing a single case design RCT (N=2)"}
#simulate some data based on a known structure
library(tidyverse)
set.seed(1981)
true_fct = stepfun(c(100, 200, 300), c(200, 250, 200, 250))

x = 1:400
y1 = true_fct(x) + rt(length(x), df=1)


TSdata <- data.frame(Time=x,y1=y1)

ggplot(data=TSdata,aes(Time, y1)) + geom_vline(xintercept = c(0,100,200,300,400),alpha=0.8,color='grey',linetype='dashed') + geom_line()+geom_point(size=0.1,alpha=0.7) + ylab('Fluency score')+theme_bw()  + annotate("text",x = c(150,350) , y =c(350,150), label = "Intervention period") 


```

In the other kind of N-of-1 approach, the multiple baseline design, it is the outcome measure that is varied. For this method to be applicable, we need an intervention that is targeted specifically at one of two or more possible behaviours. To demonstrate effectiveness, we need to show that it is the targeted behaviour that improves, while the comparison behaviour remains unaffected. For instance, @best2013 evaluated a cueing therapy for anomia in acquired aphasia in a case series of 16 patients, with the aim of comparing naming ability for 100 words that had been trained versus 100 untrained words. By using a large number of words, carefully selected to be of similar initial difficulty, they had sufficient data to clearly show whether or not there was selective improvement for the trained words in individual participants.  

### Minimising systematic bias  
We have seen in previous chapters how the RCT has evolved to minimise numerous sources of unwanted systematic bias. We need to be alert to similar biases affecting results of N-of-1 trials. This is a particular concern for ABA trials, where we compare different time periods that do or do not include intervention. On the one hand, we may have the kinds of time-linked effects of maturation or practice that lead to a general improvement over time, regardless of the intervention, and on the other hand there may be specific events that affect a person's performance, such as life events, illness, or transient environmental effects, such as noise or other distractions. Usually, these cannot be controlled, but if they are documented, then their effect can be taken into account. The general assumption of this method is that if we use a sufficient number of time intervals, time-linked biases will average out. 

### When is a N-of-1 trial feasible?  

The N-of-1 trial is sometimes dismissed as providing poor quality evidence, but it can be a highly efficient way to obtain an estimate of treatment efficacy in an individual.  However, in the context of neurorehabilitation and speech-and-language therapy, there would appear to be a major drawback in its application: in the course of a historical review of this approach, @mirza2017 described the "N-of-1 niche" as follows:  
  
_"The design is most suited to assessing interventions that act and cease to act quickly. It is particularly useful in clinical contexts in which variability in patient responses is large, when the evidence is limited, and/or when the patient differs in important ways from the people who have participated in conventional randomised controlled trials."_   
  
While the characteristics in the second sentence fit well with speech-and-language therapy interventions, the first requirement - that the intervention should "act and cease to act quickly" is clearly inapplicable. As described in the previous chapter, with few exceptions, interventions offered by speech and language therapists are intended to produce long-term change that persists long after the therapy has ended. Indeed, a therapy that worked only during the period of administration would not be regarded as a success. This means that some N-of-1 designs, notably ABA designs, which compare an outcomes for periods with and without intervention, will be inapplicable. In this regard, behavioural interventions are quite different from many pharmaceutical interventions, where ABA designs are increasingly being used to compare a series if active and washout periods for a drug.

There is however another feature of many behavioural interventions (including those used in education and clinical psychology as well as in speech and language therapy) that makes them excellent candidates for N-of-1 trials, provided we select the appropriate study design. In many cases the intervention has potential to target several specific behaviours or skills. This gives this field an edge that drug trials lack: we can change the specific outcome that is targeted and compare it with another outcome that acts as a within-person control measure, using a multiple baseline design. The example above of comparing trained vs untrained words in aphasia therapy provides a particularly straigthforward example of this approach [@best2013]. However, identifying alternative outcomes to compare is not always straightforward, so we will use a real-life example to illustrate the application.  

### Illustrative example: Effectiveness of electropalatography
We noted in the previous chapter how, electropalatography, a biofeedback intervention that provides information about the position of articulators to help clients improve production of speech sounds, is ill-suited to evaluation in a RCT. It is potentially applicable to people with a wide variety of aetiologies, it requires expensive equipment including an individualised artificial palate, and the intervention is delivered over many one-to-one sessions. The goal of the intervention is to develop and consolidate new patterns of articulation that will persist after the intervention ends. It would not, therefore, make much sense to do a N-of-1 trial of electropalatography using an ABA design that involved comparing blocks of intervention vs no intervention. One can, however, run a trial that tests whether there is more improvement on targeted speech sounds than on other speech sounds that are not explicitly treated. 

A huge benefit of the N-of-1 approach is that it can be incorporated in regular therapy sessions. It does not require large changes to the therapist's behaviour, though it does demand that a clear plan is developed in advance, and meticulous records are kept so that the effectiveness of the intervention can be evaluated.  

Leniston and Ebbels applied this approach to seven adolescents with severe speech disorders, all of whom were already familiar with electropalatography. Diagnoses included verbal dyspraxia, structural abnormalities of articulators (velopharyngeal insufficiency), mosaic Turner syndrome, and right-sided hemiplegia. At the start of each school term, two sounds were identified for each case: a target sound, which would be trained, and a control sound, which was also produced incorrectly, but which was not trained. Electropalatography training was administered twice a week in 30 minute sessions. The number of terms where intervention was given ranged from 1 to 3. 

(figure here - to recreate fig 3 from raw data if available)

An analysis of group data found no main effect of target or time, but a large interaction between these, indicating greater improvement on trained speech sounds. The design of the study made it possible to look at individual cases, which gave greater insights into variation of the impact of intervention. In the first term of intervention, there was a main effect of time for three of the participants, but no interaction with sound type. In other words, these children improved over the course of the term, but this was seen for the control as well as the target sound. By term 2, one of four children showed an interaction between time and sound type, and both children who continued training into term 3 showed such an interaction. Three children did not show any convincing evidence of benefit - all of these stopped intervention after one term. 

As the authors noted, there is a key limitation of the study: when a significant interaction is found between time and sound type, this provides evidence that the intervention was effective. But when _both_ target and control sounds improve, this is ambiguous. It could mean that the intervention was effective, and its impact generalised beyond the trained sounds. But it could also mean that the intervention was ineffective, with improvement being due to other factors, such as maturation or practice.  

There are three important lessons to be taken from this study:

First, it illustrates how an N-of-1 study can be embedded in natural therapy sessions, can include heterogeneous participants, and be adapted to fit into regular clinical practice. Where we have a therapy that can be set up to contrast a treated vs untreated target behaviour - in this case production of specific speech sounds - this allows us to set up an experimental contrast that can provide evidence on intervention effectiveness. 

Second, this method can handle the (typical) situation where intervention effects are sustained, but it is most effective if we do not expect any generalisation of learning beyond the targeted behaviour or skill. Unfortunately, this is often at odds with speech and language therapy methods. For instance, in phonological therapy, the therapist may focus on helping a child distinguish and/or produce a specific sound pair, such as [d] vs [g], but there are good theoretical reasons to expect that if therapy is successful, it might generalise to other sound pairs, such as [t] vs [k], which depend on the same articulatory contrast between alveolar vs velar place. Indeed, if we think of the child's phonology as part of a general system of contrasts, it might be expected that training on one sound pair could lead the whole system to reorganise. This is exactly what we would like to see in intervention, but it can make N-of-1 studies extremely difficult to interpret. Before designing such a study, it is worthwhile anticipating different outcomes and considering how they might be interpreted. 

Finally, one point that cannot be stressed enough is that when evaluating a given intervention, a single study is never enough. Practical constraints usually may make it impossible to devise the perfect study that gives entirely unambiguous results: rather we should aim for our studies to reduce the uncertainty in our understanding of the effectiveness of intervention, with each study building on those that have gone before. This will only happen if researchers report the full pattern of results, including those which may be ambiguous or unwanted. 

### N-of-1 case series  
Many studies assemble a series of N-of-1 cases, and regular group statistics may be applied (as was the case in the @leniston2021 study). But heterogeneity of response is expected and documented. Typically the small sample sizes preclude any strong conclusions about the characteristics of those who do and do not show intervention effects, but results may subsequently be combined across groups, and specific hypotheses formulated about the characteristics of those who show a positive response.

An example is provided in the study by @best2013 on rehabilitation for anomia in acquired aphasia. The researchers contrasted naming ability for words that had been trained, using a cueing approach, versus a set of untrained control words. In general, results were consistent with prior work in showing that improvement was largely confined to trained words. As noted above, this result allows us to draw a clear conclusion that the intervention was responsible for the improvement, but from a therapeutic perspective it was disappointing, as one might hope to see generalisation to novel words. The authors subdivided the participants according to their language profiles, and suggested that improvement on untrained words was seen in a subset of cases with a specific profile of semantic and phonological strengths. This result, however, was not striking and would need to be replicated.


<!--- the literature seems pretty murky here. There was an early paper by Howard that talked about analysing homogeneity in case series, which seems an interesting idea, but I'm not sure how valid his approach is: Howard, D. (2003). Chapter 16: Single Cases, Group Studies and Case Series in Aphasia Therapy. In The Sciences of Aphasia: From Theory to Therapy. Ilias Papathanasiou and Ria De Bleser (Eds). (pp. 245–258). Elsevier. https://doi.org/10.1016/B978-008044073-6/50017-1
https://reader.elsevier.com/reader/sd/pii/B9780080440736500171?token=402044775314B107D8FAC186B5A72020B85191FCEE862FCF95B2C037B17634D20F97DB7AFB2A1723513D3FCC21D586D4&originRegion=eu-west-1&originCreation=20210801103239.-->


### Statistical approaches to N-of-1 designs

Early reports of N-of-1 studies often focused on simple visualisation of results. This can be fine provided that differences are very obvious, as in Figure 1 above. We can think back to our discussion of analysis methods for RCTs: the aim is always to ask whether the variation associated with differences in intervention is greater than the variation within the intervention condition. In Figure 1, there is very little overlap in the values for the intervention vs non-intervention periods, and statistics are unnecessary.  However, we can simulate a more ambiguous result, simply by increasing the variation within both time periods. Figure x2 is the same as Figure x1, except that the measures are five times more variable, so the signal of the intervention is harder to see. For data such as this, we might want to support our impression of a true effect with a statistical test. The null hypothesis in this case would be that there is no difference between the four time intervals. 

Some early N-of-1 studies in neuropsychology may have drawn over-optimistic conclusions from N-of-1 studies because they had insufficient replications of outcome measures, assuming that the observed result was a valid indication of outcome without taking into account error of measurement. The more measurements we have in this type of study, the more confidence we can place in results: whereas in RCTs we need sufficient participants to get a sense of how much variation there is in outcomes, in N-of-1 studies we need sufficient observations, and should never rely just on one or two instances. 

Just as with group studies, we also have the issue of how to interpret null results. 
<!--- not sure what to say here, and we could omit this, but I have started thinking it tends to be an overlooked issue-->

```{r SingleCaseDplot2, echo=F, message=F, warning=F, fig.width=8,fig.height=4, fig.cap="plot showing a single case design RCT (N=2)"}
#simulate some data based on a known structure
library(tidyverse)
set.seed(1981)
true_fct = stepfun(c(100, 200, 300), c(200, 250, 200, 250))

x = 1:400
y1 = true_fct(x) + 5*rt(length(x), df=1)


TSdata <- data.frame(Time=x,y1=y1)

ggplot(data=TSdata,aes(Time, y1)) + geom_vline(xintercept = c(0,100,200,300,400),alpha=0.8,color='grey',linetype='dashed') + geom_line()+geom_point(size=0.1,alpha=0.7) + ylab('Fluency score')+theme_bw()  + annotate("text",x = c(150,350) , y =c(350,150), label = "Intervention period") 


```

<!---I think it would be v useful to do a bit more on this!
The paper by Leniston and Ebbels used Linear Mixed Models - Susan Ebbels said she is learning R and was not confident about it, but it seemed a plausible approach that might be preferable to the rather hodgepodgey set of things that are currently done. Your thoughts on  her methods would be v useful!-->

Statistical analysis tends to focus on: i) visual assessment by plot the sequential series of observed data and highlighting on-off periods of intervention; ii) Time series analysis [@Chatfield_2004], or iii) simple and commonly-used inferential statistics tests [@Tate_2016].




When a single case design won't work [@Senn_2004] <!--omit this? - Senn always makes a lot of sense, but is entirely focused on drug trials and ABA approach, and I think the issues are different in behavioural interventions-->


<!--The CENT reporting guidelines [67], [68] are intended for medical N-of-1 trials, the SCRIBE 2016 guidelines [22], [51] are intended for SCEDs in the behavioral sciences
Paul: I was disappointed that the SCRIBE guidelines didn't say anything about data analysis, which I think is pretty important!--> 

