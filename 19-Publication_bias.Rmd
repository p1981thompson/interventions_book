---
output:
  html_document: default
  pdf_document: default
---

#	Can we believe the literature? Publication bias and citation bias 
## Publication bias  
Imagine the situation of a researcher who conducts a study of the effectiveness of an intervention that she has developed to improve children's vocabulary. She has attended to all the sources of bias that feature in earlier chapters, and run a well-powered randomised controlled trial. But at the end of the day, the results are disappointing. Mean vocabulary scores of intervention and control groups are closely similar, with no evidence that the intervention was effective. So what happens next?  

A common response is that the researcher decides the study was a failure and files the results away. Another possibility is that they write up the research for publication, only to find that a series of journals reject the paper, because the null results are not very interesting. There is plentiful evidence that both of these things happen frequently, but does it matter? After all, it's true that null results are uninteresting: we read journals in the hope we will hear about new, effective interventions. 

In fact, failure to publish null results from well-designed studies is a massive problem for any field. This is because science is cumulative. We don't judge an intervention on a single trial: the first trial might inspire us to do further studies to get a better estimate of the effect size and generalisability of the result. But those estimates will be badly skewed if the literature is biased to contain only positive results.  

This problem was recognised decades ago. Social psychologist @greenwald1975 talked about the "Consequences of prejudice against the null hypothesis", memorably concluding that _"As it is functioning in at least some areas of behavioral science research, the research-publication system may be regarded as a device for systematically generating and propagating anecdotal information."_  
A few years later, another psychologist, @rosenthal1979a, coined the term "file drawer problem" to describe the fate of studies that were deemed insufficiently exciting to publish.

In @1976, journal editor Michael Mahoney conducted a study which would raise eyebrows in current times [@mahoney1976]. He sent 75 manuscripts out to review, but tweaked them so the results and discussion either agreed with the reviewer's viewpoint, or disagreed. A final set of reviewers was sent the paper without any results or discussion. He found that manuscripts which were identical in topic and procedure attracted favourable reviews if they found positive results, but were recommended for rejection if they found negative results. Methods that were picked apart for flaws if the findings were negative, we accepted without question when there were positive results. 

<!---START CUSTOM BLOCK-->
Mahoney's work has been largely forgotten, perhaps due to inaccessibility. It is in a book that is now out of print. His experience attempting to publish his study on publication bias, ironically, appears to demonstrate further evidence of such bias on behalf of an editor:
_"...after completing the study on the peer review system, I submitted it to Science. After several months, I received copies of the comments of three referees. One was extremely positive, opening his review with the statement that the 'paper is certainly publishable in its present form.' The other two referees were also positive - describing it as 'bold, imaginative, and interesting' - but requesting some minor revisions in the presentation and interpretation of the data. Notwithstanding these three positive reviews, Science editor Philip H. Abelson decided to reject the manuscript! Making the minor changes mentioned by the reviewers, I resubmitted the article along with a letter to Abelson noting the positive tone of the reviews and expressing puzzlement at his decision to reject. Abelson returned a three sentence letter saying (a) the manuscript 'is obviously suitable for publication in a specialized journal,' (b) if 'it were shortened it might be published as a Research Report (in Science), and (c) that I should qualify my conclusions regarding 'the area of research and publications which are covered.' It is not clear whether this latter remark was intended to imply that the peer review system in the physical sciences is not as flawed as that in the social sciences. In any case, I shortened the article, emphasised the study's limitations, and noted the researchable possibility that different results might have been obtained with a different sample of referees or in a different discipline. My efforts were rewarded several months later when Assistant Editor John E. Ringle returned the manuscript with a five sentence rejection letter, recommending that I lengthen the article and submit it to a psychology journal"._
<!---END CUSTOM BLOCK-->

Figure x shows how publication bias can work out in practice. 
![Figure from @devries2018](devriesfig.jpg)
_Figure x. The cumulative impact of reporting and citation biases on the evidence base for antidepressants. (a) displays the initial, complete cohort of trials, while (b) through (e) show the cumulative effect of biases. Each circle indicates a trial, while the color indicates the results or the presence of spin. Circles connected by a grey line indicate trials that were published together in a pooled publication. In (e), the size of the circle indicates the (relative) number of citations received by that category of studies._

In this study, the authors searched a trial registry operated by the Food and Drug Administation (FDA) for studies on drug treatments for depression. Pharmaceutical companies are required to register intervention studies in advance, which makes it possible to detect unpublished as well as published research. As can be seen from the left-most bar, around half of the trials were deemed to have 'negative' results, and the remainder found a positive effect of intervention. The second bar shows the impact of publication bias: whereas nearly all the positive studies were published, only half of those with null results made it into print.  

The next bar (c) shows yet more bias creeping in: ten negative trials switched from positive to negative, by either omitting or changing the primary study outcome. As the authors noted: "Without access to the FDA reviews, it would not have been possible to conclude that these trials, when analyzed according to protocol, were not positive." And yet other trials reported a negative result in the body of the paper, but presented the findings with a positive spin in the Abstract, either by focusing on another result, or by ignoring statistical significance. 

Given that the problems of publication bias have been recognised for decades, we may ask why they still persist - generating what @ferguson2012 has termed 'A vast graveyard of undead theories'. There appear to be two driving forces. First, traditionally journals have made their money by selling content, and publishers and editors know that people are usually far more interested in positive than in negative findings. Negative findings can be newsworthy, but only only if they challenge the effectiveness of an intervention that is widely-used and incurs costs.  [example here].  When reading the results of a positive intervention study, it is always worth asking yourself whether the study would have been accepted for publication if the results had turned out differently. For further discussion of this point see http://deevybee.blogspot.com/2013/03/high-impact-journals-where.html. 

Another force can be conflict of interest. Sometimes the developer of an intervention has the potential to gain or lose substantial amounts of income, depending on whether a trial is positive or negative. Even if there are no financial consequences, someone who has put a lot of time and effort into developing an intervention will have a strong bias towards wanting it to work. If there is a conflict of interest, it needs to be declared: of course, we should not assume that a researcher with a conflict of interest is dishonest, but there is empirical evidence that there is an association between conflict of interest and the reporting of positive results, which needs to be taken into account when reading the research literature @friedman2004. 



### Citation bias  
In the rightmost bar of Figure x, the size of the circles represents the number of citations of each paper in the literature. This illustrates another major bias that has received less attention than publication bias - citation bias. We can see that those null studies that made it through to this point, surviving with correct reporting despite publication bias, outcome reporting bias, and spin, get largely ignored, whereas studies that are reported as positive, including those which involved outcome-switching, are much more heavily cited. 

Quite apart from there being bias against null results, many researchers are not particularly thorough in their citation of prior literature. In an overview of publication bias and citation bias, @leng2020 drew attention to the neglect of prior literature in papers reporting randomised controlled trials. Based on a paper by @robinson2011, they noted _"Strangely, a median of two trials were cited regardless of how many had been conducted; seemingly, while scientists may see further by standing on the shoulders of those who have gone before, two shoulders are enough, however many are available."_ (p. 206).  

Citation bias is often unintentional, but is a consequence of the way humans think. @bishop2020d described how confirmation bias is a natural tendency that in everyday life often serves a useful purpose in reducing our cognitive load, but which is incompatible with objective scientific thinking. We find it much easier to attend to and remember things that are aligned with our prior expectations. We need to take explicit measures to counteract this tendency in order to evaluate prior literature objectively. 

### Counteracting publication and citation biases  

In chapters 20 and 21, we discuss two approaches to counteracting biases: preregistration offers a solution to publication bias, and systematic review offers a (partial) solution to citation bias.  

### Class exercise  
[Publication game!?]


Find a published report of an intervention that interests you. Take a look at the introduction, and list the references that are cited as background. Next, go online and search for articles on this topic that were published 2 or more years before the target paper. Do you find many articles that are relevant but which were not cited? If so, did they give positive or negative results? 
N.B. There are various ways you can conduct a literature search. Google Scholar is often preferred because it is free and includes a wide range of source materials, including books. Scopus and Web of Science are other commercial options that your institution may subscribe to. Both of these are more selective in coverage, which can be a positive if you want some quality control over your search (e.g. restricted to peer-reviewed journals), but a negative if you want to be comprehensive. If you compare these different ways of searching the literature, you will find they can give very different results. 


