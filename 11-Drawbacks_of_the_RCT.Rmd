# Drawbacks of the RCT

## Transfer to real life: efficacy and effectiveness

When designing a randomised controlled trial, it is important to consider the generalisation of the interventions performance from the idealised setting of an RCT to how this would perform under more pragmatic setting. Often trials make assumptions and conditions are highly controlled to ensure randomisation and experimental conditions are consistent for each participant. In practice, there is often little control over these conditions, for example, school based interventions may not adhere to the intervention timetable strictly as they may priortise or fit the intervention around other activities. 

In the clinical RCT setting, the potential mismatch between 'ideal' and 'real-world' or 'in-practice' settings has been addressed by specifically designing some early stage trials as either Explanatory trials (Efficacy) or Pragmatic trials (Effectiveness). Singal and colleagues have summarised the notable differences between these trials[@Singal_2014]. Table \@ref(tab:diffeff), reproduced from [@Singal_2014], presents the key differences as summary bullet points.
 
 
```{r diffeff,echo=FALSE,message=FALSE,warning=FALSE,results='asis'}
library(tidyverse)
library(knitr)
library(kableExtra)
options(kableExtra.html.bsTable = T)

dt <- matrix(c('Question',	'Does the intervention work under ideal circumstance?',	'Does the intervention work in real-world practice?',
'Setting',	'Resource-intensive (ideal) setting',	'Real-world everyday clinical setting',
'Study population',	'Highly selected, homogenous population (Several exclusion criteria)', 'Heterogeneous population (Few to no exclusion criteria)',
'Providers',	'Highly experienced and trained', 'Representative usual providers',
'Intervention',	'Strictly enforced and standardized. No concurrent interventions',	'Applied with flexibility. Concurrent interventions and cross-over permitted'),ncol=3,byrow=TRUE)

dt <- as.data.frame(dt)
colnames(dt) <- c('','Efficacy study',	'Effectiveness study')


knitr::kable(dt,escape = F, align = "c", booktabs = T, caption = 'Differences between efficacy and effectiveness studies') %>%
  kable_styling(c("striped", "bordered"), latex_options = "striped", full_width = F)

``` 
 
Efficacy studies are....
 
 
## Inefficiency: need for unfeasibly large samples

Historically, experimental psychology studies using relatively small samples have been the norm for decades, regardless of effect size of interest. The vast majority of these studies have the potential to be underpowered and can potentially report false positive results. The change to using larger well powered studies has been somewhat slower than medical drug trial literature as the requirements for study design have been much more flexible as less regulated. Increasingly, we see studies with smaller effect sizes appearing in the literature but study size has remained relatively low. 

In clinical RCTs, the required sample sizes can be into the thousands of individuals as the effect sizes and statistical analyses require this to give a statistical power of around 80% or more. These types of trials are ususally well funded and have a multidisciplinary team of researchers to facilitate the trial (including expert clinicians, research nurses, statisticians, data managers, field interviewers|qualitative researchers,expert groups, and data entry clerks). When designing an RCT in education or psychology, the resource constraints, whether regarding time, available researcher assistants, or financial, can determine whether the trial is even feasible. 

Effect sizes of interventions are frequently over estimated in the planning stages of an RCT due to optimism for the new intervention that the researcher has developed, or using information to judge an intervention that itself has inherent bias, for example small pilot studies or relying on reported effect sizes from a literature that is populated with underpowered studies or suffers from publication bias , i.e. omission of null results.

Even when the research intervention has been well-planned and the effect size estimate is reasonably accurate, for example, has used simulation or the smallest effect size of interest, the estimates can be pragmatically limiting. For example, if a study's population of interest is a rare condition or a particularly difficult to recruit condition. Other populations of interest may be highly variable and study designs needed to deal with this might also require additional individuals or larger numbers of obserations. 


## Heterogeneity and personalised intervention

An RCT by design will assess the 'average' effect of the intervention observed in a study sample. Generally, the traditional RCT is not able to provide information on the effectiveness regarding specific individuals. If the RCT has been randomised correctly, then the identification of individuals 'on treatment' or 'control' is not possible without breaking the blinding. Hence, we can only discuss results in terms of average difference or effect size at the group level. Figure \@ref(fig:interventionhet) shows a hypothetical longitudinal parallel-group study. Average linear regression lines of intervention and controls group line is shown as solid black and dotted lines respectively. We can see that all individuals are plotted as separate coloured lines (red for control and blue for intervention). However, we highlight the average effect that is usually reported in black lines and can clearly see the variation in both groups around those averages. In some cases in the intervention group, the intervention did appear to make a difference, however, because we do not know what their reponse would have been if they were in the control group, then we have no ideal reference to judge the personalised effect.   

```{r interventionhet, echo=FALSE,message=FALSE,warning=FALSE,fig.cap="shows a hypothetical longitudinal parallel-group study. Average linear regression lines of intervention and controls group line is shown as solid black and dotted lines respectively."}
library(ggplot2)
 
data(ChickWeight)
ChickWeight2 <- subset(ChickWeight, Diet == 1|Diet == 2)
ChickWeight2$Intervention <- ChickWeight2$Diet
levels(ChickWeight2$Intervention) <- c("Control","Intervention")
ChickWeight2$Outcome <-ChickWeight2$weight

reg1 <- lm(Outcome ~ Time, data = subset(ChickWeight2, Diet == 1))
reg2 <- lm(Outcome ~ Time, data = subset(ChickWeight2, Diet == 2))

predicted_df1 <- data.frame(Outcome = predict(reg1, subset(ChickWeight2, Diet == 1)), Time=subset(ChickWeight2, Diet == 1)$Time, Chick=subset(ChickWeight2, Diet == 1)$Chick)
predicted_df2 <- data.frame(Outcome = predict(reg2, subset(ChickWeight2, Diet == 2)), Time=subset(ChickWeight2, Diet == 2)$Time,Chick=subset(ChickWeight2, Diet == 2)$Chick)

ggplot(data=ChickWeight2,aes(x = Time, y = Outcome, group = Chick)) + geom_line(aes(colour=Intervention),alpha=0.5) +
    geom_point(aes(colour=Intervention),alpha=0.5) + theme_bw() + geom_line(data = predicted_df1, aes(x=Time, y=Outcome),linetype='dashed') + geom_line(data = predicted_df2, aes(x=Time, y=Outcome))


```


## Yet more bias: distortions arising at the analysis stage


### Dangers of too many outcome measures
 
Clinical intervention RCTs are usually registered for a single outcome measure of interest to limit the amount of data dredging for an effect that is loosely related to the hypothesis of interest (typically, 'is the intervention effective?'). Secondary outcome measures are also specificed but reporting of analyses relating to these outcomes is often much more exploratory. This idea of fishing or data dredging is important in hypothesis testing (as we have seen in Chapter 9, multiple hypothesis testing) using more than one outcome measure, as the chance of finding a statistically significant results increases linearly with the amount of extra statistical tests that are performed. As we have previously seen, there are a number of correction that can be applied to adjust the type I error rate or the family wise error rate. 

Beyond clinical intervention research, behavioural and psychological intervention research is currently more flexible in the amount of outcomes that are recorded at testing and the amount that are actually appear documented in the final dissemination of results. This flexibility is often referred to as P-hacking [@Munafo_2017]. Preregistration of hypotheses and analyses prior to seeing the data help to alleviate the problem of questionable research practices, such as P-hacking or HARKing (hypothesising after results known). 

In addition, too many outcome measures can substantially complicate the statistical analyses required to evaluate the research question of interest. It might be the case that the researcher requires multiple outcomes to determine the effectiveness of an intervention, for example, a quantity of interest might not be able to be measured directly, so several proxy measures are recorded to inform a composite outcome measure. In this case, a more complex multivariate analysis is needed, but this in turn may require a substantially greater sample size. 

Finally, we may find that using many outcomes in a multiple testing framework using corrections can be suboptimal for discovering effects as many corrections can be conservative (i.e. the corrected significance level can be much harder to find an effect of interest, and actually increase the type II errors). 
 
### Dangers of post-hoc subgroups and covariates

A related issue posed by using 'too many outcomes', posthoc analyses conducted after seeing the data can be highly misleading to both the researcher and the intended audience unless reported correctly as exploratory analyses (also referred to as hypothesis generating). If we look at the data and observed interesting patterns of data, then subset those individuals based on these observations, we may find that we pursue chance findings rather than a true phenomenon. This is most commonly found when analysing some data with a particular analysis in mind, the researcher may get a result that are against their original hyptothesis; however, they do observe that their hypothesis might be supported if a covaraiate is used to adjust the analysis or if a subgroup of particular individuals is analysed instead. This pattern of data exploration is, in principle, justfied provided that the complete analysis timeline is presented and 'badged' as exploratory. When this process cherry picks the analyses and results that favour their hypothesis wthout documenting all analyses conducted, this is another example of P-hacking.

A related concept is 